{
    "id": "d4d4ddf0c860fefb552c40aefaccecbe",
    "metadata": {
        "id": "d4d4ddf0c860fefb552c40aefaccecbe",
        "url": "https://docs.zenml.io/concepts/steps_and_pipelines",
        "title": "Steps & Pipelines | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Steps and Pipelines are the core building blocks of ZenML",
            "keywords": null,
            "author": null,
            "og:title": "Steps & Pipelines | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Steps and Pipelines are the core building blocks of ZenML",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/WXp4FRa4pzeaxgBLrW0n",
            "twitter:card": "summary_large_image",
            "twitter:title": "Steps & Pipelines | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Steps and Pipelines are the core building blocks of ZenML",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/WXp4FRa4pzeaxgBLrW0n"
        }
    },
    "content": "`Ctrl``k`\n\nGitBook AssistantAsk\n\nProductResourcesGitHubStart free\n\nMore\n\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\n\nGitBook Assistant\n\nGitBook Assistant\n\nWorking...Thinking...\n\nGitBook Assistant\n\n##### Good evening\n\nI'm here to help you with the docs.\n\nWhat is this page about?What should I read next?Can you give an example?\n\n`Ctrl``i`\n\nAI Based on your context\n\nSend\n\n  * Getting Started\n\n    * Welcome to ZenML\n    * Installation\n    * Hello World\n    * Your First AI Pipeline\n    * Core Concepts\n    * System Architecture\n  * Deploying ZenML\n\n    * Deploy\n    * Connect\n    * Manage\n  * Concepts\n\n    * Steps & Pipelines\n\n      * Configuration\n      * Scheduling\n      * Logging\n      * YAML Configuration\n      * Source Code and Imports\n      * Advanced Features\n      * Dynamic Pipelines (Experimental)\n\n    * Artifacts\n    * Stack & Components\n    * Service Connectors\n    * Pipeline Snapshots\n    * Pipeline Deployments\n    * Containerization\n    * Code Repositories\n    * Secrets\n    * Environment Variables\n    * Tags\n    * Metadata\n    * Models\n    * Dashboard\n    * Templates\n  * Reference\n\n    * Community & content\n    * Environment Variables\n    * MCP Docs & llms.txt\n    * FAQ\n    * Global settings\n    * Legacy docs\n\n\n\nPowered by GitBook\n\nOn this page\n\n  * The Relationship Between Steps and Pipelines\n  * Basic Steps\n  * Creating a Simple Step\n  * Step Inputs and Outputs\n  * Custom Output Names\n  * Basic Pipelines\n  * Creating a Simple Pipeline\n  * Running Pipelines\n  * End-to-End Example\n  * Parameters and Artifacts\n  * Understanding the Difference\n  * Parameter Types\n  * Parameterizing Workflows\n  * Step Parameterization\n  * Pipeline Parameterization\n  * Step Type Handling & Output Management\n  * Type Annotations\n  * Multiple Return Values\n  * Conclusion\n\n\n\nWas this helpful?\n\nGitBook AssistantAsk\n\n  1. Concepts\n\n\n\n# Steps & Pipelines\n\nSteps and Pipelines are the core building blocks of ZenML\n\nSteps and Pipelines are the fundamental building blocks of ZenML. A **Step** is a reusable unit of computation, and a **Pipeline** is a directed acyclic graph (DAG) composed of steps. Together, they allow you to define, version, and execute machine learning workflows.\n\n## \n\nThe Relationship Between Steps and Pipelines\n\nIn ZenML, steps and pipelines work together in a clear hierarchy:\n\n  1. **Steps** are individual functions that perform specific tasks, like loading data, processing it, or training models\n\n  2. **Pipelines** orchestrate these steps, connecting them in a defined sequence where outputs from one step can flow as inputs to others\n\n  3. Each step produces artifacts that are tracked, versioned, and can be reused across pipeline runs\n\n\n\n\nThink of a step as a single LEGO brick, and a pipeline as the complete structure you build by connecting many bricks together.\n\n## \n\nBasic Steps\n\n### \n\nCreating a Simple Step\n\nA step is created by applying the `@step` decorator to a Python function:\n\nCopy```\nfrom zenml import step\n@step\ndefload_data() ->dict:\n  training_data = [[1,2], [3,4], [5,6]]\n  labels = [0,1,0]\nreturn{'features': training_data,'labels': labels}\n```\n\n\n### \n\nStep Inputs and Outputs\n\nSteps can take inputs and produce outputs. These can be simple types, complex data structures, or custom objects.\n\nCopy```\n@step\ndef process_data(data: dict) -> dict:\n  # Input: data dictionary with features and labels\n  # Process the input data\n  processed_features = [feature * 2 for feature in data['features']]\n  # Output: return processed data and statistics\n  return {\n    'processed_features': processed_features,\n    'labels': data['labels'],\n    'num_samples': len(data['features']),\n    'feature_sum': sum(map(sum, data['features']))\n  }\n```\n\n\nIn this example:\n\n  * The step takes a `dict` as input containing features and labels\n\n  * It processes the features and computes some statistics\n\n  * It returns a new `dict` as output with the processed data and additional information\n\n\n\n\n### \n\nCustom Output Names\n\nYou can name your step outputs using the `Annotated` type:\n\nCopy```\nfrom typing import Annotated\nfrom typing import Tuple\n@step\ndef divide(a: int, b: int) -> Tuple[\n  Annotated[int, \"quotient\"],\n  Annotated[int, \"remainder\"]\n]:\n  return a // b, a % b\n```\n\n\nBy default, step outputs are named `output` for single output steps and `output_0`, `output_1`, etc. for steps with multiple outputs.\n\n## \n\nBasic Pipelines\n\n### \n\nCreating a Simple Pipeline\n\nA pipeline is created by applying the `@pipeline` decorator to a Python function that composes steps together:\n\nCopy```\nfrom zenml import pipeline\n@pipeline\ndef simple_ml_pipeline():\n  dataset = load_data()\n  train_model(dataset)\n```\n\n\n### \n\nRunning Pipelines\n\nYou can run a pipeline by simply calling the function:\n\nCopy```\nsimple_ml_pipeline()\n```\n\n\nThe run is automatically logged to the ZenML dashboard where you can view the DAG or Timeline view and associated metadata.\n\n## \n\nEnd-to-End Example\n\nHere's a simple end-to-end example that demonstrates the basic workflow:\n\nCopy```\nimport numpy as np\nfrom typing import Tuple\nfrom zenml import step, pipeline\n# Create steps for a simple ML workflow\n@step\ndef get_data() -> Tuple[np.ndarray, np.ndarray]:\n  # Generate some synthetic data\n  X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n  y = np.array([0, 1, 0, 1])\n  return X, y\n@step\ndef process_data(data: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n  X, y = data\n  # Apply a simple transformation\n  X_processed = X * 2\n  return X_processed, y\n@step\ndef train_and_evaluate(processed_data: Tuple[np.ndarray, np.ndarray]) -> float:\n  X, y = processed_data\n  # Simplistic \"training\" - just compute accuracy based on a rule\n  predictions = [1 if sum(sample) > 10 else 0 for sample in X]\n  accuracy = sum(p == actual for p, actual in zip(predictions, y)) / len(y)\n  return accuracy\n# Create a pipeline that combines these steps\n@pipeline\ndef simple_example_pipeline():\n  raw_data = get_data()\n  processed_data = process_data(raw_data)\n  accuracy = train_and_evaluate(processed_data)\n  print(f\"Model accuracy: {accuracy}\")\n# Run the pipeline\nif __name__ == \"__main__\":\n  simple_example_pipeline()\n```\n\n\n## \n\nParameters and Artifacts\n\n### \n\nUnderstanding the Difference\n\nZenML distinguishes between two types of inputs to steps:\n\n  1. **Artifacts** : Outputs from other steps in the same pipeline\n\n     * These are tracked, versioned, and stored in the artifact store\n\n     * They are passed between steps and represent data flowing through your pipeline\n\n     * Examples: datasets, trained models, evaluation metrics\n\n  2. **Parameters** : Direct values provided when invoking a step\n\n     * These are typically simple configuration values passed directly to the step\n\n     * They're not tracked as separate artifacts but are recorded with the pipeline run\n\n     * Examples: learning rates, batch sizes, model hyperparameters\n\n\n\n\nThis example demonstrates the difference:\n\nCopy```\n@pipeline\ndef my_pipeline():\n  int_artifact = some_other_step() # This is an artifact\n  # input_1 is an artifact, input_2 is a parameter\n  my_step(input_1=int_artifact, input_2=42)\n```\n\n\n### \n\nParameter Types\n\nParameters can be:\n\n  1. **Primitive types** : `int`, `float`, `str`, `bool`\n\n  2. **Container types** : `list`, `dict`, `tuple` (containing primitives)\n\n  3. **Custom types** : As long as they can be serialized to JSON using Pydantic\n\n\n\n\nParameters that cannot be serialized to JSON should be passed as artifacts rather than parameters.\n\n## \n\nParameterizing Workflows\n\n### \n\nStep Parameterization\n\nSteps can take parameters like regular Python functions:\n\nCopy```\n@step\ndef train_model(data: dict, learning_rate: float = 0.01, epochs: int = 10) -> None:\n  # Use learning_rate and epochs parameters\n  print(f\"Training with learning rate: {learning_rate} for {epochs} epochs\")\n```\n\n\n### \n\nPipeline Parameterization\n\nPipelines can also be parameterized, allowing values to be passed down to steps:\n\nCopy```\n@pipeline\ndef training_pipeline(dataset_name: str = \"default_dataset\", learning_rate: float = 0.01):\n  data = load_data(dataset_name=dataset_name)\n  train_model(data=data, learning_rate=learning_rate, epochs=20)\n```\n\n\nYou can then run the pipeline with specific parameters:\n\nCopy```\ntraining_pipeline(dataset_name=\"custom_dataset\", learning_rate=0.005)\n```\n\n\n## \n\nStep Type Handling & Output Management\n\n### \n\nType Annotations\n\nWhile optional, type annotations are highly recommended and provide several benefits:\n\n  * **Artifact handling** : ZenML uses type annotations to determine how to serialize, store, and load artifacts. The type information guides ZenML to select the appropriate materializer for saving and loading step outputs.\n\n  * **Type validation** : ZenML validates inputs against type annotations at runtime to catch errors early.\n\n  * **Code documentation** : Types make your code more self-documenting and easier to understand.\n\n\n\n\nCopy```\nfrom typing import Tuple\n@step\ndef square_root(number: int) -> float:\n  return number ** 0.5\n@step\ndef divide(a: int, b: int) -> Tuple[int, int]:\n  return a // b, a % b\n```\n\n\nWhen you specify a return type like `-> float` or `-> Tuple[int, int]`, ZenML uses this information to determine how to store the step's output in the artifact store. For instance, a step returning a pandas DataFrame with the annotation `-> pd.DataFrame` will use the pandas-specific materializer for efficient storage.\n\nIf you want to enforce type annotations for all steps, set the environment variable `ZENML_ENFORCE_TYPE_ANNOTATIONS` to `True`.\n\n### \n\nMultiple Return Values\n\nSteps can return multiple artifacts:\n\nCopy```\nfrom typing import Tuple\nfrom sklearn.base import ClassifierMixin\nfrom typing import Annotated\n@step\ndef train_classifier(X_train, y_train) -> Tuple[\n  Annotated[ClassifierMixin, \"model\"],\n  Annotated[float, \"accuracy\"]\n]:\n  model = SVC(gamma=0.001)\n  model.fit(X_train, y_train)\n  accuracy = model.score(X_train, y_train)\n  return model, accuracy\n```\n\n\nZenML uses the following convention to differentiate between a single output of type `Tuple` and multiple outputs:\n\n  * When the `return` statement is followed by a tuple literal (e.g., `return 1, 2` or `return (value_1, value_2)`), it's treated as a step with multiple outputs\n\n  * All other cases are treated as a step with a single output of type `Tuple`\n\n\n\n\n## \n\nConclusion\n\nSteps and Pipelines provide a flexible, powerful way to build machine learning workflows in ZenML. This guide covered the basic concepts of creating steps and pipelines, managing inputs and outputs, and working with parameters.\n\nFor more advanced features, check out the Advanced Features guide. For configuration using YAML files, see Configuration with YAML.\n\nPreviousMigration guide 0.58.2 â†’ 0.60.0NextConfiguration\n\nLast updated 2 months ago\n\nWas this helpful?\n",
    "summary": "## TL;DR Summary of ZenML Steps & Pipelines\n\n### Core Concepts\n- **Steps**: Reusable computation units (e.g., data loading, processing).\n- **Pipelines**: Directed acyclic graphs (DAGs) that orchestrate steps, allowing outputs from one step to serve as inputs to another.\n\n### Creating Steps\n- Use the `@step` decorator to define a step.\n- Steps can take inputs and produce outputs, which can be simple types or complex structures.\n\n### Creating Pipelines\n- Use the `@pipeline` decorator to compose steps.\n- Run pipelines by calling the function, with automatic logging to the ZenML dashboard.\n\n### Parameters and Artifacts\n- **Artifacts**: Outputs from steps, tracked and versioned.\n- **Parameters**: Direct values provided to steps, not tracked as artifacts.\n\n### Type Annotations\n- Recommended for artifact handling, validation, and documentation.\n- Steps can return multiple values using tuples.\n\n### Conclusion\nSteps and Pipelines in ZenML enable flexible machine learning workflows, with a focus on modularity and reusability. For advanced features, refer to the Advanced Features guide.",
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines/configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/scheduling",
        "https://docs.zenml.io/concepts/steps_and_pipelines/logging",
        "https://docs.zenml.io/concepts/steps_and_pipelines/yaml_configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/sources",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/concepts/steps_and_pipelines#the-relationship-between-steps-and-pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines#basic-steps",
        "https://docs.zenml.io/concepts/steps_and_pipelines#creating-a-simple-step",
        "https://docs.zenml.io/concepts/steps_and_pipelines#step-inputs-and-outputs",
        "https://docs.zenml.io/concepts/steps_and_pipelines#custom-output-names",
        "https://docs.zenml.io/concepts/steps_and_pipelines#basic-pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines#creating-a-simple-pipeline",
        "https://docs.zenml.io/concepts/steps_and_pipelines#running-pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines#end-to-end-example",
        "https://docs.zenml.io/concepts/steps_and_pipelines#parameters-and-artifacts",
        "https://docs.zenml.io/concepts/steps_and_pipelines#understanding-the-difference",
        "https://docs.zenml.io/concepts/steps_and_pipelines#parameter-types",
        "https://docs.zenml.io/concepts/steps_and_pipelines#parameterizing-workflows",
        "https://docs.zenml.io/concepts/steps_and_pipelines#step-parameterization",
        "https://docs.zenml.io/concepts/steps_and_pipelines#pipeline-parameterization",
        "https://docs.zenml.io/concepts/steps_and_pipelines#step-type-handling-and-output-management",
        "https://docs.zenml.io/concepts/steps_and_pipelines#type-annotations",
        "https://docs.zenml.io/concepts/steps_and_pipelines#multiple-return-values",
        "https://docs.zenml.io/concepts/steps_and_pipelines#conclusion",
        "https://docs.zenml.io/concepts",
        "https://docs.zenml.io/concepts/dashboard-features#timeline-view",
        "https://docs.zenml.io/concepts/artifacts/materializers",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server/migration-guide/migration-zero-sixty",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/?utm_source=content&utm_medium=trademark&utm_campaign=5aBlTJNbVDkrxJp7J1J9"
    ]
}