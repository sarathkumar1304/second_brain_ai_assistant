{
    "id": "67bebbdf3c0bddffcd939cfbe98cece6",
    "metadata": {
        "id": "67bebbdf3c0bddffcd939cfbe98cece6",
        "url": "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines",
        "title": "Dynamic Pipelines (Experimental) | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Write dynamic pipelines",
            "keywords": null,
            "author": null,
            "og:title": "Dynamic Pipelines (Experimental) | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Write dynamic pipelines",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/3hvfl6YZAIlw0FjhscPg",
            "twitter:card": "summary_large_image",
            "twitter:title": "Dynamic Pipelines (Experimental) | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Write dynamic pipelines",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/3hvfl6YZAIlw0FjhscPg"
        }
    },
    "content": "`Ctrl``k`\n\nGitBook AssistantAsk\n\nProductResourcesGitHubStart free\n\nMore\n\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\n\nGitBook Assistant\n\nGitBook Assistant\n\nWorking...Thinking...\n\nGitBook Assistant\n\n##### Good evening\n\nI'm here to help you with the docs.\n\nWhat is this page about?What should I read next?Can you give an example?\n\n`Ctrl``i`\n\nAI Based on your context\n\nSend\n\n  * Getting Started\n\n    * Welcome to ZenML\n    * Installation\n    * Hello World\n    * Your First AI Pipeline\n    * Core Concepts\n    * System Architecture\n  * Deploying ZenML\n\n    * Deploy\n    * Connect\n    * Manage\n  * Concepts\n\n    * Steps & Pipelines\n\n      * Configuration\n      * Scheduling\n      * Logging\n      * YAML Configuration\n      * Source Code and Imports\n      * Advanced Features\n      * Dynamic Pipelines (Experimental)\n\n    * Artifacts\n    * Stack & Components\n    * Service Connectors\n    * Pipeline Snapshots\n    * Pipeline Deployments\n    * Containerization\n    * Code Repositories\n    * Secrets\n    * Environment Variables\n    * Tags\n    * Metadata\n    * Models\n    * Dashboard\n    * Templates\n  * Reference\n\n    * Community & content\n    * Environment Variables\n    * MCP Docs & llms.txt\n    * FAQ\n    * Global settings\n    * Legacy docs\n\n\n\nPowered by GitBook\n\nOn this page\n\n  * Why Dynamic Pipelines?\n  * Basic Example\n  * Key Features\n  * Dynamic Step Configuration\n  * Step Runtime Configuration\n  * Map/Reduce over collections\n  * Parallel Step Execution\n  * Config Templates with depends_on\n  * Pass pipeline parameters when running snapshots from the server\n  * Limitations and Known Issues\n  * Logging\n  * Error Handling\n  * Orchestrator Support\n  * Artifact Loading\n  * Mapping Limitations\n  * Best Practices\n  * When to Use Dynamic Pipelines\n\n\n\nWas this helpful?\n\nGitBook AssistantAsk\n\n  1. Concepts\n  2. Steps & Pipelines\n\n\n\n# Dynamic Pipelines (Experimental)\n\nWrite dynamic pipelines\n\n**Experimental Feature** : Dynamic pipelines are currently an experimental feature. There are known issues and limitations, and the interface is subject to change. This feature is only supported by the `local`, `kubernetes`, `sagemaker` and `vertex` orchestrators. If you encounter any issues or have feedback, please let us know at https://github.com/zenml-io/zenml/issues.\n\n**Important** : Before using dynamic pipelines, please review the Limitations and Known Issues section below. This section contains critical information about requirements and known bugs that may affect your pipeline execution, especially when running remotely.\n\n## \n\nWhy Dynamic Pipelines?\n\nTraditional ZenML pipelines require you to define the entire DAG structure at pipeline definition time. While this works well for many use cases, there are scenarios where you need more flexibility:\n\n  * **Runtime-dependent workflows** : When the number of steps or their configuration depends on data computed during pipeline execution\n\n  * **Dynamic parallelization** : When you need to spawn multiple parallel step executions based on runtime conditions\n\n  * **Conditional execution** : When the workflow structure needs to adapt based on intermediate results\n\n\n\n\nDynamic pipelines allow you to write pipelines that generate their DAG structure dynamically at runtime, giving you the power of Python's control flow (loops, conditionals) combined with ZenML's orchestration capabilities.\n\n## \n\nBasic Example\n\nThe simplest dynamic pipeline uses regular Python control flow to determine step execution:\n\nCopy```\nfrom zenml import step, pipeline\n@step\ndefgenerate_int() ->int:\nreturn3\n@step\ndefdo_something(index:int) ->None:\nprint(f\"Processing index {index}\")\n@pipeline(dynamic=True)\ndefdynamic_pipeline() ->None:\n  count =generate_int()\n# `count` is an artifact, we now load the data\n  count_data = count.load()\nfor idx inrange(count_data):\n# This will run sequentially, like regular Python code would.\ndo_something(idx)\nif__name__==\"__main__\":\ndynamic_pipeline()\n```\n\n\nIn this example, the number of `do_something` steps executed depends on the value returned by `generate_int()`, which is only known at runtime.\n\n## \n\nKey Features\n\n### \n\nDynamic Step Configuration\n\nYou can configure steps dynamically within your pipeline using `with_options()`:\n\nCopy```\n@pipeline(dynamic=True)\ndef dynamic_pipeline():\n  some_step.with_options(enable_cache=False)()\n```\n\n\nThis allows you to modify step behavior based on runtime conditions or data.\n\n### \n\nStep Runtime Configuration\n\nYou can control where a step executes by specifying its runtime:\n\n  * `**runtime=\"inline\"**`: The step runs in the orchestration environment (same process/container as the orchestrator)\n\n  * `**runtime=\"isolated\"**`: The orchestrator spins up a separate step execution environment (new container/process)\n\n\n\n\nCopy```\n@step(runtime=\"isolated\")\ndef some_step() -> None:\n  # This step will run in its own isolated environment\n  ...\n@step(runtime=\"inline\")\ndef another_step() -> None:\n  # This step will run in the orchestration environment\n  ...\n```\n\n\nUse `runtime=\"isolated\"` when you need:\n\n  * Better resource isolation\n\n  * Different environment requirements\n\n  * Parallel execution (see below)\n\n\n\n\nUse `runtime=\"inline\"` when you need:\n\n  * Faster execution (no container startup overhead)\n\n  * Shared resources with the orchestrator\n\n  * Sequential execution\n\n\n\n\n### \n\nMap/Reduce over collections\n\nDynamic pipelines support a high-level map/reduce pattern over sequence-like step outputs. This lets you fan out a step across items of a collection and then reduce the results without manually writing loops or loading data in the orchestration environment.\n\nCopy```\nfrom zenml import pipeline, step\n@step\ndef producer() -> list[int]:\n  return [1, 2, 3]\n@step\ndef worker(value: int) -> int:\n  return value * 2\n@step\ndef reducer(values: list[int]) -> int:\n  return sum(values)\n@pipeline(dynamic=True, enable_cache=False)\ndef map_reduce():\n  values = producer()\n  results = worker.map(values)  # fan out over collection\n  reducer(results)        # pass list of artifacts directly\n```\n\n\nKey points:\n\n  * `step.map(...)` fans out a step over sequence-like inputs.\n\n  * Steps can accept lists of artifacts directly as inputs (useful for reducers).\n\n  * You can pass the mapped output directly to a downstream step without loading in the orchestration environment.\n\n\n\n\n#### \n\nMapping semantics: map vs product\n\n  * `step.map(...)`: If multiple sequence-like inputs are provided, all must have the same length `n`. ZenML creates `n` mapped steps where the i-th step receives the i-th element from each input.\n\n  * `step.product(...)`: Creates a mapped step for each combination of elements across all input sequences (cartesian product).\n\n\n\n\nExample (cartesian product):\n\nCopy```\nfrom zenml import pipeline, step\n@step\ndef int_values() -> list[int]:\n  return [1, 2]\n@step\ndef str_values() -> list[str]:\n  return [\"a\", \"b\", \"c\"]\n@step\ndef do_something(a: int, b: str) -> int:\n  ...\n@pipeline(dynamic=True)\ndef cartesian_example():\n  a = int_values()\n  b = str_values()\n  # Produces 2 * 3 = 6 mapped steps\n  combine.product(a, b)\n```\n\n\n#### \n\nBroadcasting inputs with unmapped(...)\n\nIf you want to pass a sequence-like artifact as a whole to each mapped invocation (i.e., avoid splitting), wrap it with `unmapped(...)`:\n\nCopy```\nfrom zenml import pipeline, step, unmapped\n@step\ndef producer(length: int) -> list[int]:\n  return [1] * length\n@step\ndef consumer(a: int, b: list[int]) -> None:\n  # `b` is the full list for every mapped call\n  ...\n@pipeline(dynamic=True)\ndef unmapped_example():\n  a = producer(length=3)  # list of 3 ints\n  b = producer(length=4)  # list of 4 ints\n  consumer.map(a=a, b=unmapped(b))\n```\n\n\n#### \n\nUnpacking mapped outputs\n\nIf a mapped step returns multiple outputs, you can split them into separate lists (one per output) using `unpack()`. This returns a tuple of lists of artifact futures, aligned by mapped invocation.\n\nCopy```\nfrom zenml import pipeline, step\n@step\ndef create_int_list() -> list[int]:\n  return [1, 2]\n@step\ndef compute(a: int) -> tuple[int, int]:\n  return a * 2, a * 3\n@pipeline(dynamic=True)\ndef map_pipeline():\n  ints = create_int_list()\n  results = compute.map(a=ints) # Map over [1, 2]\n  # Unpack per-output across all mapped invocations\n  double, triple = results.unpack()\n  # Each element is an ArtifactFuture; load to get concrete values\n  doubles = [f.load() for f in double] # [2, 4]\n  triples = [f.load() for f in triple] # [3, 6]\n```\n\n\nNotes:\n\n  * `results` is a future that refers to all outputs of all steps, and `unpack()` works for both `.map(...)` and `.product(...)`.\n\n  * Each list contains future objects that refer to a single artifact.\n\n\n\n\n### \n\nParallel Step Execution\n\nDynamic pipelines support true parallel execution using `step.submit()`. This method returns a `StepRunFuture` that you can use to wait for results or pass to downstream steps:\n\nCopy```\nfrom zenml import step, pipeline\n@step\ndef some_step(arg: int) -> int:\n  return arg * 2\n@pipeline(dynamic=True)\ndef dynamic_pipeline():\n  # Submit a step for parallel execution\n  future = some_step.submit(arg=1)\n  # Wait and get artifact response(s)\n  artifact = future.result()\n  # Wait and load artifact data\n  data = future.load()\n  # Pass the output to another step\n  downstream_step(future)\n  # Run multiple steps in parallel\n  for idx in range(3):\n    some_step.submit(arg=idx)\n```\n\n\nThe `StepRunFuture` object provides several methods:\n\n  * `**result()**`: Wait for the step to complete and return the artifact response(s)\n\n  * `**load()**`: Wait for the step to complete and load the actual artifact data\n\n  * **Pass directly** : You can pass a `StepRunFuture` directly to downstream steps, and ZenML will automatically wait for it\n\n\n\n\nWhen using `step.submit()`, steps with `runtime=\"isolated\"` will execute in separate containers/processes, while steps with `runtime=\"inline\"` will execute in separate threads within the orchestration environment.\n\n### \n\nConfig Templates with `depends_on`\n\nYou can use YAML configuration files to provide default parameters for steps using the `depends_on` parameter:\n\nCopy```\n# config.yaml\nsteps:\n some_step:\n  parameters:\n   arg: 3\n```\n\n\nCopy```\n# run.py\nfrom zenml import step, pipeline\n@step\ndef some_step(arg: int) -> None:\n  print(f\"arg is {arg}\")\n@pipeline(dynamic=True, depends_on=[some_step])\ndef dynamic_pipeline():\n  some_step()\nif __name__ == \"__main__\":\n  dynamic_pipeline.with_options(config_path=\"config.yaml\")()\n```\n\n\nThe `depends_on` parameter tells ZenML which steps can be configured via the YAML file. This is particularly useful when you want to allow users to configure pipeline behavior without modifying code.\n\n### \n\nPass pipeline parameters when running snapshots from the server\n\nWhen running a snapshot from the server (either via the UI or the SDK/Rest API), you can now pass pipeline parameters for your dynamic pipelines.\n\nFor example:\n\nCopy```\nfrom zenml.client import Client\nClient().trigger_pipeline(snapshot_id=<ID>, run_configuration={\"parameters\": {\"my_param\": 3}})\n```\n\n\n## \n\nLimitations and Known Issues\n\n### \n\nLogging\n\nOur logging storage isn't threadsafe yet, which means logs from parallel steps may be mixed up when multiple steps execute concurrently. This is a known limitation that we're working to address.\n\n### \n\nError Handling\n\nWhen running multiple steps concurrently using `step.submit()`, a failure in one step does not automatically stop other steps. Instead, they continue executing until finished. You should implement your own error handling logic if you need coordinated failure behavior.\n\n### \n\nOrchestrator Support\n\nDynamic pipelines are currently only supported by:\n\n  * `local` orchestrator\n\n  * `kubernetes` orchestrator\n\n  * `sagemaker` orchestrator\n\n  * `vertex` orchestrator\n\n\n\n\nOther orchestrators will raise an error if you try to run a dynamic pipeline with them.\n\n### \n\nArtifact Loading\n\nWhen you call `.load()` on an artifact in a dynamic pipeline, it synchronously loads the data. For large artifacts or when you want to maintain parallelism, consider passing the step outputs (future or artifact) directly to downstream steps instead of loading them.\n\n### \n\nMapping Limitations\n\n  * Mapping is currently supported only over artifacts produced within the same pipeline run (mapping over raw data or external artifacts is not supported).\n\n  * Chunk size for mapped collection loading defaults to 1 and is not yet configurable.\n\n\n\n\n## \n\nBest Practices\n\n  1. **Use**`**runtime=\"isolated\"**`**for parallel steps** : This ensures better resource isolation and prevents interference between concurrent step executions.\n\n  2. **Handle step outputs appropriately** : If you need the data immediately, use `.load()`. If you're just passing to another step, pass the output directly.\n\n  3. **Be mindful of resource usage** : Running many steps in parallel can consume significant resources. Monitor your orchestrator's resource limits.\n\n  4. **Test incrementally** : Start with simple dynamic pipelines and gradually add complexity. Dynamic pipelines can be harder to debug than static ones.\n\n  5. **Use config templates for flexibility** : The `depends_on` feature allows you to make pipelines configurable without code changes.\n\n\n\n\n## \n\nWhen to Use Dynamic Pipelines\n\nDynamic pipelines are ideal for:\n\n  * **AI agent orchestration** : Coordinating multiple autonomous agents (e.g., retrieval or reasoning agents) whose interactions or number of invocations are determined at runtime\n\n  * **Hyperparameter tuning** : Spawning multiple training runs with different configurations\n\n  * **Data processing** : Processing variable numbers of data chunks in parallel\n\n  * **Conditional workflows** : Adapting pipeline structure based on runtime data\n\n  * **Dynamic batching** : Creating batches based on available data\n\n  * **Multi-agent and collaborative AI workflows** : Building flexible, adaptive workflows where agents or LLM-driven components can be dynamically spawned, routed, or looped based on outputs, results, or user input\n\n\n\n\nFor most standard ML workflows, traditional static pipelines are simpler and more maintainable. Use dynamic pipelines when you specifically need runtime flexibility that static pipelines cannot provide.\n\nPreviousAdvanced FeaturesNextArtifacts\n\nLast updated 11 days ago\n\nWas this helpful?\n",
    "summary": "## Dynamic Pipelines (Experimental)\n\n### Overview\nDynamic pipelines in ZenML allow for flexible pipeline structures that adapt at runtime, supporting scenarios like runtime-dependent workflows, dynamic parallelization, and conditional execution. This feature is experimental and has known limitations.\n\n### Key Features\n- **Dynamic Step Configuration**: Modify step behavior using `with_options()`.\n- **Step Runtime Configuration**: Control execution environment with `runtime=\"inline\"` or `runtime=\"isolated\"`.\n- **Map/Reduce**: High-level patterns for processing collections without manual loops.\n- **Parallel Execution**: Use `step.submit()` for true parallel execution of steps.\n\n### Limitations\n- Logging is not thread-safe.\n- Error handling must be implemented manually for concurrent steps.\n- Supported only by specific orchestrators (local, Kubernetes, SageMaker, Vertex).\n\n### Best Practices\n- Use `runtime=\"isolated\"` for parallel steps.\n- Handle outputs appropriately and monitor resource usage.\n- Test incrementally and utilize config templates for flexibility.\n\n### Use Cases\nIdeal for AI orchestration, hyperparameter tuning, data processing, and dynamic workflows. Traditional static pipelines are recommended for standard ML workflows.",
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines/configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/scheduling",
        "https://docs.zenml.io/concepts/steps_and_pipelines/logging",
        "https://docs.zenml.io/concepts/steps_and_pipelines/yaml_configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/sources",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#why-dynamic-pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#basic-example",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#key-features",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#dynamic-step-configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#step-runtime-configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#map-reduce-over-collections",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#parallel-step-execution",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#config-templates-with-depends_on",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#pass-pipeline-parameters-when-running-snapshots-from-the-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#limitations-and-known-issues",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#logging",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#error-handling",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#orchestrator-support",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#artifact-loading",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#mapping-limitations",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#best-practices",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#when-to-use-dynamic-pipelines",
        "https://docs.zenml.io/concepts",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#mapping-semantics-map-vs-product",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#broadcasting-inputs-with-unmapped",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines#unpacking-mapped-outputs",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/?utm_source=content&utm_medium=trademark&utm_campaign=5aBlTJNbVDkrxJp7J1J9",
        "https://github.com/zenml-io/zenml/issues"
    ]
}