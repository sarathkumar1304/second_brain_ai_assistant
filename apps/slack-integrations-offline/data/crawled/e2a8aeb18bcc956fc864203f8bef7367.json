{
    "id": "e2a8aeb18bcc956fc864203f8bef7367",
    "metadata": {
        "id": "e2a8aeb18bcc956fc864203f8bef7367",
        "url": "https://docs.zenml.io/concepts/deployment/deployment_settings",
        "title": "Deployment Settings | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Customize the pipeline deployment ASGI application with DeploymentSettings.",
            "keywords": null,
            "author": null,
            "og:title": "Deployment Settings | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Customize the pipeline deployment ASGI application with DeploymentSettings.",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/rNyYUWhOPv0v5B4MnxqZ",
            "twitter:card": "summary_large_image",
            "twitter:title": "Deployment Settings | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Customize the pipeline deployment ASGI application with DeploymentSettings.",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/rNyYUWhOPv0v5B4MnxqZ"
        }
    },
    "content": "`Ctrl``k`\n\nGitBook AssistantAsk\n\nProductResourcesGitHubStart free\n\nMore\n\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\n\nGitBook Assistant\n\nGitBook Assistant\n\nWorking...Thinking...\n\nGitBook Assistant\n\n##### Good evening\n\nI'm here to help you with the docs.\n\nWhat is this page about?What should I read next?Can you give an example?\n\n`Ctrl``i`\n\nAI Based on your context\n\nSend\n\n  * Getting Started\n\n    * Welcome to ZenML\n    * Installation\n    * Hello World\n    * Your First AI Pipeline\n    * Core Concepts\n    * System Architecture\n  * Deploying ZenML\n\n    * Deploy\n    * Connect\n    * Manage\n  * Concepts\n\n    * Steps & Pipelines\n    * Artifacts\n    * Stack & Components\n    * Service Connectors\n    * Pipeline Snapshots\n    * Pipeline Deployments\n\n      * Deployment Settings\n\n    * Containerization\n    * Code Repositories\n    * Secrets\n    * Environment Variables\n    * Tags\n    * Metadata\n    * Models\n    * Dashboard\n    * Templates\n  * Reference\n\n    * Community & content\n    * Environment Variables\n    * MCP Docs & llms.txt\n    * FAQ\n    * Global settings\n    * Legacy docs\n\n\n\nPowered by GitBook\n\nOn this page\n\n  * Deployment servers and ASGI apps\n  * Configuration overview\n  * Basic customization options\n  * Advanced customization options\n  * Implementation customizations for advanced use cases\n\n\n\nWas this helpful?\n\nGitBook AssistantAsk\n\n  1. Concepts\n  2. Pipeline Deployments\n\n\n\n# Deployment Settings\n\nCustomize the pipeline deployment ASGI application with DeploymentSettings.\n\n### \n\nDeployment servers and ASGI apps\n\nZenML pipeline deployments run an ASGI application under a production-grade `uvicorn` server. This makes your pipelines callable over HTTP for online workloads like real-time ML inference, LLM agents/workflows, and even full web apps co-located with pipelines.\n\nAt runtime, three core components work together:\n\n  * the ASGI application: the HTTP surface that exposes endpoints (health, invoke, metrics, docs) and any custom routes or middleware you configure. This is powered by an ASGI framework like FastAPI, Starlette, Django, Flask, etc.\n\n  * the ASGI application factory (aka the Deployment App Runner): this component is responsible for constructing the ASGI application piece by piece based on the instructions provided by users via runtime configuration.\n\n  * the Deployment Service: the component responsible for the business logic that backs the pipeline deployment and its invocation lifecycle.\n\n\n\n\nBoth the Deployment App Runner and the Deployment Service are customizable at runtime, through the `DeploymentSettings` configuration mechanism. They can also be extended via inheritance to support different ASGI frameworks or to tweak existing functionality.\n\nThe `DeploymentSettings` class lets you shape both server behavior and the ASGI app composition without changing framework code. Typical reasons to customize include:\n\n  * Tight security posture: CORS controls, strict headers, authentication, API surface minimization.\n\n  * Observability: request/response logging, tracing, metrics, correlation identifiers.\n\n  * Enterprise integration: policy gateways, SSO/OIDC/OAuth, audit logging, routing and network architecture constraints.\n\n  * Product UX: single-page application (SPA) static files served alongside deployment APIs or custom docs paths.\n\n  * Performance/SRE: thread pool sizing, uvicorn worker settings, log levels, max request sizes and platform-specific fine-tuning.\n\n\n\n\nAll `DeploymentSettings` are pipeline-level settings. They apply to the deployment that serves the pipeline as a whole. They are not available at step-level.\n\n### \n\nConfiguration overview\n\nYou can configure `DeploymentSettings` in Python or via YAML, the same way as other settings classes. The settings can be attached to a pipeline decorator or via `with_options`. These settings are only valid at pipeline level.\n\n#### \n\nPython configuration\n\nUse the `DeploymentSettings` class to configure the deployment settings for your pipeline in-code\n\nCopy```\nfrom zenml import pipeline\nfrom zenml.config import DeploymentSettings\ndeploy_settings = DeploymentSettings(\n  app_title=\"Fraud Scoring Service\",\n  app_description=(\n    \"Online scoring API exposing synchronous and batch inference\"\n  ),\n  app_version=\"1.2.0\",\n  root_url_path=\"\",\n  api_url_path=\"\",\n  docs_url_path=\"/docs\",\n  redoc_url_path=\"/redoc\",\n  invoke_url_path=\"/invoke\",\n  health_url_path=\"/health\",\n  info_url_path=\"/info\",\n  metrics_url_path=\"/metrics\",\n  cors={\n    \"allow_origins\": [\"https://app.example.com\"],\n    \"allow_methods\": [\"GET\", \"POST\", \"OPTIONS\"],\n    \"allow_headers\": [\"*\"],\n    \"allow_credentials\": True,\n  },\n  thread_pool_size=32,\n  uvicorn_host=\"0.0.0.0\",\n  uvicorn_port=8080,\n  uvicorn_workers=2,\n)\n@pipeline(settings={\"deployment\": deploy_settings})\ndef scoring_pipeline() -> None:\n  ...\n# Alternatively\nscoring_pipeline = scoring_pipeline.with_options(\n  settings={\"deployment\": deploy_settings}\n)\n```\n\n\n#### \n\nYAML configuration\n\nDefine settings in a YAML configuration file for better separation of code and configuration:\n\nCopy```\nsettings:\n deployment:\n  app_title: Fraud Scoring Service\n  app_description: >-\n   Online scoring API exposing synchronous and batch inference\n  app_version: \"1.2.0\"\n  root_url_path: \"\"\n  api_url_path: \"\"\n  docs_url_path: \"/docs\"\n  redoc_url_path: \"/redoc\"\n  invoke_url_path: \"/invoke\"\n  health_url_path: \"/health\"\n  info_url_path: \"/info\"\n  metrics_url_path: \"/metrics\"\n  cors:\n   allow_origins: [\"https://app.example.com\"]\n   allow_methods: [\"GET\", \"POST\", \"OPTIONS\"]\n   allow_headers: [\"*\"]\n   allow_credentials: true\n  thread_pool_size: 32\n  uvicorn_host: 0.0.0.0\n  uvicorn_port: 8080\n  uvicorn_workers: 2\n```\n\n\nCheck out this page for more information on the hierarchy and precedence of the various ways in which you can supply the settings.\n\n### \n\nBasic customization options\n\n`DeploymentSettings` expose the following basic customization options. The sections below provide short examples and guidance.\n\n  * application metadata and paths\n\n  * built-in endpoints and middleware toggles\n\n  * static files (SPAs) and dashboards\n\n  * CORS\n\n  * secure headers\n\n  * startup and shutdown hooks\n\n  * uvicorn server options, logging level, and thread pool size\n\n\n\n\n#### \n\nApplication metadata\n\nYou can set `app_title`, `app_description`, and `app_version` to be reflected in the ASGI application's metadata:\n\nCopy```\nfrom zenml.config import DeploymentSettings\nsettings = DeploymentSettings(\n  app_title=\"LLM Agent Service\",\n  app_description=(\n    \"Agent endpoints for tools, state inspection, and tracing\"\n  ),\n  app_version=\"0.7.0\",\n)\n```\n\n\n#### \n\nDefault URL paths, endpoints and middleware\n\nThe ASGI application exposes the following built-in endpoints by default:\n\n  * documentation endpoints:\n\n    * `/docs` - The OpenAPI documentation UI generated based on the endpoints and their signatures.\n\n    * `/redoc` - The ReDoc documentation UI generated based on the endpoints and their signatures.\n\n  * REST API endpoints:\n\n    * `/invoke` - The main pipeline invocation endpoint for synchronous inference.\n\n    * `/health` - The health check endpoint.\n\n    * `/info` - The info endpoint providing extensive information about the deployment and its service.\n\n    * `/metrics` - Simple metrics endpoint.\n\n  * dashboard endpoints - present only if the accompanying UI is enabled:\n\n    * `/`, `/index.html`, `/static` - Endpoints for serving the dashboard files from the `dashboard_files_path` directory.\n\n\n\n\nThe ASGI application includes the following built-in middleware by default:\n\n  * secure headers middleware: for setting security headers.\n\n  * CORS middleware: for handling CORS requests.\n\n\n\n\nYou can include or exclude these default endpoints and middleware either globally or individually by setting the `include_default_endpoints` and `include_default_middleware` settings. It is also possible to remap the built-in endpoint URL paths.\n\nCopy```\nfrom zenml.config import (\n  DeploymentSettings,\n  DeploymentDefaultEndpoints,\n  DeploymentDefaultMiddleware,\n)\nsettings = DeploymentSettings(\n  # Include only the endpoints you need\n  include_default_endpoints=(\n    DeploymentDefaultEndpoints.DOCS\n    | DeploymentDefaultEndpoints.INVOKE\n    | DeploymentDefaultEndpoints.HEALTH\n  ),\n  # Customize the root URL path\n  root_url_path=\"/pipeline\",\n  # Include only the middleware you need\n  include_default_middleware=DeploymentDefaultMiddleware.CORS,\n  # Customize the base API URL path used for all REST API endpoints\n  api_url_path=\"/api\",\n  # Customize the documentation URL path\n  docs_url_path=\"/documentation\",\n  # Customize the health check URL path\n  health_url_path=\"/healthz\",\n)\n```\n\n\nWith the above settings, the ASGI application will only expose the following endpoints and middleware:\n\n  * `/pipeline/documentation` - The API documentation (OpenAPI schema)\n\n  * `/pipeline/api/invoke` - The REST API pipeline invocation endpoint\n\n  * `/pipeline/api/healthz` - The REST API health check endpoint\n\n  * CORS middleware: for handling CORS requests\n\n\n\n\n#### \n\nStatic files (single-page applications)\n\nDeployed pipelines can serve full single-page applications (React/Vue/Svelte) from the same origin as your inference API. This eliminates CORS/auth/routing friction and lets you ship user-facing UI components alongside your endpoints, such as:\n\n  * operator dashboards\n\n  * governance portals\n\n  * experiment browsers\n\n  * feature explorers\n\n  * custom data labeling interfaces\n\n  * model cards\n\n  * observability dashboards\n\n  * customer-facing playgrounds\n\n\n\n\nCo-locating UI and API streamlines delivery (one image, one URL, one CI/CD), improves latency, and keeps telemetry and auth consistent.\n\nTo enable this, point `dashboard_files_path` to a directory containing an `index.html` and any static assets. The path must be relative to the source root:\n\nCopy```\nsettings = DeploymentSettings(\n  dashboard_files_path=\"web/build\" # contains index.html and assets/\n)\n```\n\n\nA rudimentary playground dashboard is included with the ZenML python package that features a simple UI useful for sending pipeline invocations and viewing the pipeline's response.\n\nWhen supplying your own custom dashboard, you may also need to customize the security headers to allow the dashboard to access various resources. For example, you may want to tweak the `Content-Security-Policy` header to allow the dashboard to access external javascript libraries, images, etc.\n\n**Jinja2 templates**\n\nYou can use a Jinja2 template to dynamically generate the `index.html` file that hosts the single-page application. This is useful if you want to dynamically generate the dashboard files based on the pipeline configuration, step configuration or stack configuration. A `service_info` variable is passed to the template that contains the service information, such as the service name, version, and description. This variable has the same structure as the `zenml.deployers.server.models.ServiceInfo` model.\n\nExample:\n\nCopy```\n<html>\n<head>\n  <title>Pipeline: {{ service_info.pipeline.pipeline_name }}</title>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <link rel=\"stylesheet\" href=\"https://unpkg.com/mvp.css\">\n</head>\n<body>\n  <h1>Pipeline: {{ service_info.pipeline.pipeline_name }}</h1>\n  <p>Deployment: {{ service_info.deployment.name }}</p>\n</body>\n</html>\n```\n\n\n#### \n\nCORS\n\nFine-tune cross-origin access:\n\nCopy```\nfrom zenml.config import DeploymentSettings, CORSConfig\nsettings = DeploymentSettings(\n  cors=CORSConfig(\n    allow_origins=[\"https://app.example.com\", \"https://admin.example.com\"],\n    allow_methods=[\"GET\", \"POST\", \"OPTIONS\"],\n    allow_headers=[\"authorization\", \"content-type\", \"x-request-id\"],\n    allow_credentials=True,\n  )\n)\n```\n\n\n#### \n\nSecure headers\n\nHarden responses with strict headers. Each field supports either a boolean or string. Using `True` selects a safe default, `False` disables the header, and custom strings allow fully custom policies:\n\nCopy```\nfrom zenml.config import (\n  DeploymentSettings,\n  SecureHeadersConfig,\n)\nsettings = DeploymentSettings(\n  secure_headers=SecureHeadersConfig(\n    server=True, # emit default ZenML server header value\n    hsts=True,  # default: 63072000; includeSubdomains\n    xfo=True,   # default: SAMEORIGIN\n    content=True, # default: nosniff\n    csp=(\n      \"default-src 'none'; connect-src 'self' https://api.example.com; \"\n      \"img-src 'self' data:; style-src 'self' 'unsafe-inline'\"\n    ),\n    referrer=True,\n    cache=True,\n    permissions=True,\n  )\n)\n```\n\n\nSet any field to `False` to omit that header. Set to a string for a custom value. The defaults are strong, production-safe policies.\n\n#### \n\nStartup and shutdown hooks\n\nLifecycle startup and shutdown hooks are called as part of the ASGI application's lifespan. This is an alternative to the `on_init` and `on_cleanup` hooks that can be configured at pipeline level.\n\nCommon use-cases:\n\n  * Model inference\n\n    * load models/tokenizers and warm caches (JIT/ONNX/TensorRT, HF, sklearn)\n\n    * hydrate feature stores, connect to vector DBs (FAISS, Milvus, PGVector)\n\n    * initialize GPU memory pools and thread/process pools\n\n    * set global config, download artifacts from registry or object store\n\n    * prefetch embeddings, label maps, lookup tables\n\n    * create connection pools for databases, Redis, Kafka, SQS, Pub/Sub\n\n  * LLM agent workflows\n\n    * initialize LLM client(s), tool registry, and router/policy engine\n\n    * build or load RAG indexes; warm retrieval caches and prompts\n\n    * configure rate limiting, concurrency guards, circuit breakers\n\n    * load guardrails (PII filters, toxicity, jailbreak detection)\n\n    * configure tracing/observability for token usage and tool calls\n\n  * Shutdown\n\n    * flush metrics/traces/logs, close pools/clients, persist state/caches\n\n    * graceful draining: wait for in-flight requests before teardown\n\n\n\n\nHooks can be provided as:\n\n  * A Python callable object\n\n  * A source path string to be loaded dynamically (e.g. `my_project.runtime.hooks.on_startup`)\n\n\n\n\nThe callable must accept an `app_runner` argument of type `BaseDeploymentAppRunner` and any additional keyword arguments. The `app_runner` argument is the application factory that is responsible for building the ASGI application. You can use it to access information such as:\n\n  * the ASGI application instance that is being built\n\n  * the deployment service instance that is being deployed\n\n  * the `DeploymentResponse` object itself, which also contains details about the snapshot, pipeline, etc.\n\n\n\n\nCopy```\nfrom zenml.deployers.server import BaseDeploymentAppRunner\ndef on_startup(app_runner: BaseDeploymentAppRunner, warm: bool = False) -> None:\n  # e.g., warm model cache, connect tracer, prefetch embeddings\n  ...\ndef on_shutdown(app_runner: BaseDeploymentAppRunner, drain_timeout_s: int = 2) -> None:\n  # e.g., flush metrics, close clients\n  ...\nsettings = DeploymentSettings(\n  startup_hook=on_startup,\n  shutdown_hook=on_shutdown,\n  startup_hook_kwargs={\"warm\": True},\n  shutdown_hook_kwargs={\"drain_timeout_s\": 2},\n)\n```\n\n\nYAML using source strings:\n\nCopy```\nsettings:\n deployment:\n  startup_hook: my_project.runtime.hooks.on_startup\n  shutdown_hook: my_project.runtime.hooks.on_shutdown\n  startup_hook_kwargs:\n   warm: true\n  shutdown_hook_kwargs:\n   drain_timeout_s: 2\n```\n\n\n#### \n\nUvicorn and threading\n\nTune server runtime parameters for performance and topology:\n\nThe following settings are available for tuning the uvicorn server:\n\n  * `thread_pool_size`: the size of the thread pool for CPU-bound work offload.\n\n  * `uvicorn_host`: the host to bind the uvicorn server to.\n\n  * `uvicorn_port`: the port to bind the uvicorn server to.\n\n  * `uvicorn_workers`: the number of workers to use for the uvicorn server.\n\n  * `log_level`: the log level to use for the uvicorn server.\n\n  * `uvicorn_reload`: whether to enable auto-reload for the uvicorn server. This is useful when using the local Deployer stack component to speed up local development by automatically restarting the server when code changes are detected. NOTE: the `uvicorn_reload` setting has no effect on changes in the pipeline configuration, step configuration or stack configuration.\n\n  * `uvicorn_kwargs`: a dictionary of keyword arguments to pass to the uvicorn server.\n\n\n\n\nThe following settings are available:\n\nCopy```\nfrom zenml.config import DeploymentSettings\nfrom zenml.enums import LoggingLevels\nsettings = DeploymentSettings(\n  thread_pool_size=64, # CPU-bound work offload\n  uvicorn_host=\"0.0.0.0\",\n  uvicorn_port=8000,\n  uvicorn_workers=2,  # multi-process model\n  log_level=LoggingLevels.INFO,\n  uvicorn_kwargs={\n    \"proxy_headers\": True,\n    \"forwarded_allow_ips\": \"*\",\n    \"timeout_keep_alive\": 15,\n  },\n)\n```\n\n\n### \n\nAdvanced customization options\n\nWhen the built-in ASGI application, endpoints and middleware are not enough, you can take customizing your deployment to the next level by providing your own implementation for endpoints, middleware and other ASGI application extensions. ZenML `DeploymentSettings` provides a flexible and extensible mechanism to inject your own custom code into the ASGI application at runtime:\n\n  * custom endpoints - to expose your own HTTP endpoints.\n\n  * custom middleware - to insert your own ASGI middleware.\n\n  * free-form ASGI application building extensions - to take full control of the ASGI application and its lifecycle for truly advanced use-cases when endpoints and middleware are not enough.\n\n\n\n\n#### \n\nCustom endpoints\n\nIn production, custom endpoints are often required alongside the main pipeline invoke route. Common use-cases include:\n\n  * Online inference controls\n\n    * model (re)load, warm-up, and cache priming\n\n    * dynamic model/version switching and traffic shaping (A/B, canary)\n\n    * async/batch prediction submission and job-status polling\n\n    * feature store materialization/backfills and online/offline sync triggers\n\n  * Enterprise integration\n\n    * authentication bootstrap (API key issuance/rotation), JWKS rotation\n\n    * OIDC/OAuth device-code flows and SSO callback handlers\n\n    * external system webhooks (CRM, billing, ticketing, audit sink)\n\n  * Observability and operations\n\n    * detailed health/readiness endpoints (subsystems, dependencies)\n\n    * metrics/traces/log shipping toggles; log level switch (INFO/DEBUG)\n\n    * maintenance-mode enable/disable and graceful drain controls\n\n  * LLM agent serving\n\n    * tool registry CRUD, tool execution sandboxes, guardrail toggles\n\n    * RAG index CRUD (upsert documents, rebuild embeddings, vacuum/compact)\n\n    * prompt template catalogs and runtime overrides\n\n    * session memory inspection/reset, conversation export/import\n\n  * Governance and data management\n\n    * payload redaction policy updates and capture sampling controls\n\n    * schema/contract discovery (sample payloads, test vectors)\n\n    * tenant provisioning, quotas/limits, and per-tenant configuration\n\n\n\n\nYou can configure `custom_endpoints` in `DeploymentSettings` to expose your own HTTP endpoints.\n\nEndpoints support multiple definition modes (see code examples below):\n\n  1. Direct callable - a simple function that takes in request parameters and returns a response. Framework-specific arguments such as FastAPI's `Request`, `Response` and dependency injection patterns are supported.\n\n  2. Builder class - a callable class with a `__call__` method that is the actual endpoint callable described at 1). The builder class constructor is called by the ASGI application factory and can be leveraged to execute any global initialization logic before the endpoint is called.\n\n  3. Builder function - a function that returns the actual endpoint callable described at 1). Similar to the builder class.\n\n  4. Native framework-specific object (`native=True`). This can vary from ASGI framework to framework.\n\n\n\n\nDefinitions can be provided as Python objects or as loadable source path strings.\n\nThe builder class and builder function must accept an `app_runner` argument of type `BaseDeploymentAppRunner`. This is the application factory that is responsible for building the ASGI application. You can use it to access information such as:\n\n  * the ASGI application instance that is being built\n\n  * the deployment service instance that is being deployed\n\n  * the `DeploymentResponse` object itself, which also contains details about the snapshot, pipeline, etc.\n\n\n\n\nThe final endpoint callable can take any input arguments and return any output that are JSON-serializable or Pydantic models. The application factory will handle converting these into the appropriate schema for the ASGI application.\n\nYou can also use framework-specific request/response types (e.g. FastAPI `Request`, `Response`) or dependency injection patterns for your endpoint callable if needed. However, this will limit the portability of your endpoint to other frameworks.\n\nThe following code examples demonstrate the different definition modes for custom endpoints:\n\n  1. a custom detailed health check endpoint implemented as a direct callable\n\n\n\n\nCopy```\nfrom typing import Any, Callable, Dict, List\nfrom pydantic import BaseModel\nfrom zenml.client import Client\nfrom zenml.config import (\n  DeploymentSettings,\n  EndpointSpec,\n  EndpointMethod,\n)\nfrom zenml.deployers.server import BaseDeploymentAppRunner\nfrom zenml.models import DeploymentResponse\nasync def health_detailed() -> Dict[str, Any]:\n  import psutil\n  client = Client()\n  return {\n    \"status\": \"healthy\",\n    \"cpu_percent\": psutil.cpu_percent(),\n    \"memory_percent\": psutil.virtual_memory().percent,\n    \"disk_percent\": psutil.disk_usage(\"/\").percent,\n    \"zenml\": client.zen_store.get_store_info().model_dump(),\n  }\nsettings = DeploymentSettings(\n  custom_endpoints=[\n    EndpointSpec(\n      path=\"/health\",\n      method=EndpointMethod.GET,\n      handler=health_detailed,\n      auth_required=False,\n    ),\n  ]\n)\n```\n\n\n  1. a custom ML model inference endpoint, implemented as a builder function. Note how the builder function loads the model only once at runtime, and then reuses it for all subsequent requests.\n\n\n\n\nCopy```\nfrom typing import Any, Callable, Dict, List\nfrom pydantic import BaseModel\nfrom zenml.client import Client\nfrom zenml.config import (\n  DeploymentSettings,\n  EndpointSpec,\n  EndpointMethod,\n)\nfrom zenml.deployers.server import BaseDeploymentAppRunner\nfrom zenml.models import DeploymentResponse\nclass PredictionRequest(BaseModel):\n  features: List[float]\nclass PredictionResponse(BaseModel):\n  prediction: float\n  confidence: float\ndef build_predict_endpoint(\n  app_runner: BaseDeploymentAppRunner,\n  model_name: str,\n  model_version: str,\n  model_artifact: str,\n) -> Callable[[PredictionRequest], PredictionResponse]:\n  stored_model_version = Client().get_model_version(model_name, model_version)\n  stored_model_artifact = stored_model_version.get_artifact(model_artifact)\n  model = stored_model_artifact.load()\n  def predict(\n    request: PredictionRequest,\n  ) -> PredictionResponse:\n    pred = float(model.predict([request.features])[0])\n    # Example: return fixed confidence if model lacks proba\n    return PredictionResponse(prediction=pred, confidence=0.9)\n  return predict\nsettings = DeploymentSettings(\n  custom_endpoints=[\n    EndpointSpec(\n      path=\"/predict/custom\",\n      method=EndpointMethod.POST,\n      handler=build_predict_endpoint,\n      init_kwargs={\n        \"model_name\": \"fraud-classifier\",\n        \"model_version\": \"v1\",\n        \"model_artifact\": \"sklearn_model\",\n      },\n      auth_required=True,\n    ),\n  ]\n)\n```\n\n\nNOTE: a similar way to do this is to implement a proper ZenML pipeline that loads the model in the `on_init` hook and then runs pre-processing and inference steps in the pipeline.\n\n  1. a custom deployment info endpoint implemented as a builder class\n\n\n\n\nCopy```\nfrom typing import Any, Awaitable, Callable, Dict, List\nfrom pydantic import BaseModel\nfrom zenml.client import Client\nfrom zenml.config import (\n  DeploymentSettings,\n  EndpointSpec,\n  EndpointMethod,\n)\nfrom zenml.deployers.server import BaseDeploymentAppRunner\nfrom zenml.models import DeploymentResponse\ndef build_deployment_info(app_runner: BaseDeploymentAppRunner) -> Callable[[], Awaitable[DeploymentResponse]]:\n  async def endpoint() -> DeploymentResponse:\n    return app_runner.deployment\n  return endpoint\nsettings = DeploymentSettings(\n  custom_endpoints=[\n    EndpointSpec(\n      path=\"/deployment\",\n      method=EndpointMethod.GET,\n      handler=build_deployment_info,\n      auth_required=True,\n    ),\n  ]\n)\n```\n\n\n  1. a custom model selection endpoint, implemented as a FastAPI router. This example is more involved and demonstrates how to coordinate multiple endpoints with the main pipeline invoke endpoint.\n\n\n\n\nCopy```\n# my_project.fastapi_endpoints\nfrom __future__ import annotations\nfrom typing import List, Optional\nfrom fastapi import APIRouter, HTTPException, status\nfrom pydantic import BaseModel, Field\nfrom sklearn.base import ClassifierMixin\nfrom zenml.client import Client\nfrom zenml.models import ArtifactVersionResponse\nfrom zenml.config import DeploymentSettings, EndpointSpec, EndpointMethod\nmodel_router = APIRouter()\n# Global, process-local model registry for inference\nCURRENT_MODEL: Optional[Any] = None\nCURRENT_MODEL_ARTIFACT: Optional[ArtifactVersionResponse] = None\nclass LoadModelRequest(BaseModel):\n  \"\"\"Request to load/replace the in-memory model version.\"\"\"\n  model_name: str = Field(default=\"fraud-classifier\")\n  version_name: str = Field(default=\"v1\")\n  artifact_name: str = Field(default=\"sklearn_model\")\n@model_router.post(\"/load\", response_model=ArtifactVersionResponse)\ndef load_model(req: LoadModelRequest) -> ArtifactVersionResponse:\n  \"\"\"Load or replace the in-memory model version.\"\"\"\n  global CURRENT_MODEL, CURRENT_MODEL_ARTIFACT\n  model_version = Client().get_model_version(\n    req.model_name, req.version_name\n  )\n  CURRENT_MODEL_ARTIFACT = model_version.get_artifact(req.artifact_name)\n  CURRENT_MODEL = CURRENT_MODEL_ARTIFACT.load()\n  return CURRENT_MODEL_ARTIFACT\n@model_router.get(\"/current\", response_model=ArtifactVersionResponse)\ndef current_model() -> ArtifactVersionResponse:\n  \"\"\"Return the artifact of the currently loaded in-memory model.\"\"\"\n  if CURRENT_MODEL_ARTIFACT is None:\n    raise HTTPException(\n      status_code=status.HTTP_404_NOT_FOUND,\n      detail=\"No model loaded. Use /model/load first.\",\n    )\n  return CURRENT_MODEL_ARTIFACT\ndeploy_settings = DeploymentSettings(\n  custom_endpoints=[\n    EndpointSpec(\n      path=\"/model\",\n      method=EndpointMethod.POST, # method is ignored for native routers\n      handler=\"my_project.fastapi_endpoints.model_router\",\n      native=True,\n      auth_required=True,\n    )\n  ]\n)\n```\n\n\nAnd here is a minimal ZenML inference pipeline that uses the globally loaded model. The prediction step reads the model from the global variable set by the FastAPI router above. You can invoke this pipeline via the built-in `/invoke` endpoint once a model has been loaded through `/model/load`.\n\nCopy```\nfrom typing import List\nfrom pydantic import BaseModel\nfrom zenml import pipeline, step\nclass InferenceRequest(BaseModel):\n  features: List[float]\nclass InferenceResponse(BaseModel):\n  prediction: float\n@step\ndef preprocess_step(request: InferenceRequest) -> List[float]:\n  # Replace with real transformations, scaling, encoding, etc.\n  return request.features\n@step\ndef predict_step(features: List[float]) -> InferenceResponse:\n  \"\"\"Run model inference using the globally loaded model.\"\"\"\n  if GLOBAL_CURRENT_MODEL is None:\n    raise RuntimeError(\n      \"No model loaded. Call /model/load before invoking.\"\n    )\n  pred = float(GLOBAL_CURRENT_MODEL.predict([features])[0])\n  return InferenceResponse(prediction=pred)\n@pipeline(settings={\"deployment\": deploy_settings})\ndef inference_pipeline(request: InferenceRequest) -> InferenceResponse:\n  processed = preprocess_step(request)\n  return predict_step(processed)\n```\n\n\n#### \n\nCustom middleware\n\nMiddleware is where you enforce cross-cutting concerns consistently across every endpoint. Common use-cases include:\n\n  * Security and access control\n\n    * API key/JWT verification, tenant extraction and context injection\n\n    * IP allow/deny lists, basic WAF-style request filtering, mTLS header checks\n\n    * Request body/schema validation and max body size enforcement\n\n  * Governance and privacy\n\n    * PII detection/redaction on inputs/outputs; payload sampling/scrubbing\n\n    * Policy enforcement (data residency, retention, consent) at request time\n\n  * Reliability and traffic shaping\n\n    * Rate limiting, quotas, per-tenant concurrency limits\n\n    * Idempotency keys, deduplication, retries with backoff, circuit breakers\n\n    * Timeouts, slow-request detection, maintenance mode and graceful drain\n\n  * Observability\n\n    * Correlation/trace IDs, OpenTelemetry spans, structured logging\n\n    * Metrics for latency, throughput, error rates, request/response sizes\n\n  * Performance and caching\n\n    * Response caching/ETags, compression (gzip/br), streaming/chunked responses\n\n    * Adaptive content negotiation and serialization tuning\n\n  * LLM/agent-specific controls\n\n    * Token accounting/limits, cost guards per tenant/user\n\n    * Guardrails (toxicity/PII/jailbreak) and output filtering\n\n    * Tool execution sandboxing gates and allowlists\n\n  * Data and feature enrichment\n\n    * Feature store prefetch, user/tenant profile enrichment, AB bucketing tags\n\n\n\n\nYou can configure `custom_middlewares` in `DeploymentSettings` to insert your own ASGI middleware.\n\nMiddlewares support multiple definition modes (see code examples below):\n\n  1. Middleware class - a standard ASGI middleware class that implements the `__call__` method that takes the traditional `scope`, `receive` and `send` arguments. The constructor must accept an `app` argument of type `ASGIApplication` and any additional keyword arguments.\n\n  2. Middleware callable - a callable that takes all arguments in one go: `app`, `scope`, `receive` and `send`.\n\n  3. Native framework-specific middleware (`native=True`) - this can vary from ASGI framework to framework.\n\n\n\n\nDefinitions can be provided as Python objects or as loadable source path strings. The `order` parameter controls the insertion order in the middleware chain. Lower `order` values insert the middleware earlier in the chain.\n\nThe following code examples demonstrate the different definition modes for custom middlewares:\n\n  1. a custom middleware that adds a processing time header to every response, implemented as a middleware class:\n\n\n\n\nCopy```\nimport time\nfrom typing import Any\nfrom asgiref.compatibility import guarantee_single_callable\nfrom asgiref.typing import (\n  ASGIApplication,\n  ASGIReceiveCallable,\n  ASGISendCallable,\n  ASGISendEvent,\n  Scope,\n)\nfrom zenml.config import DeploymentSettings, MiddlewareSpec\nclass RequestTimingMiddleware:\n  \"\"\"ASGI middleware to measure request processing time.\"\"\"\n  def __init__(self, app: ASGIApplication, header_name: str = \"x-process-time-ms\") -> None:\n    self.app = guarantee_single_callable(app)\n    self.header_name = header_name\n  async def __call__(\n    self,\n    scope: Scope,\n    receive: ASGIReceiveCallable,\n    send: ASGISendCallable,\n  ) -> None:\n    if scope[\"type\"] != \"http\":\n      await self.app(scope, receive, send)\n      return\n    start_time = time.time()\n    async def send_wrapper(message: ASGISendEvent) -> None:\n      if message[\"type\"] == \"http.response.start\":\n        process_time = (time.time() - start_time) * 1000\n        headers = list(message.get(\"headers\", []))\n        headers.append((self.header_name.encode(), str(process_time).encode()))\n        message = {**message, \"headers\": headers}\n      await send(message)\n    await self.app(scope, receive, send_wrapper)\nsettings = DeploymentSettings(\n  custom_middlewares=[\n    MiddlewareSpec(\n      middleware=RequestTimingMiddleware,\n      order=10,\n      init_kwargs={\"header_name\": \"x-process-time-ms\"},\n    ),\n  ]\n)\n```\n\n\n  1. a custom middleware that injects a correlation ID into responses (and generates one if missing), implemented as a middleware callable:\n\n\n\n\nCopy```\nimport uuid\nfrom typing import Any\nfrom asgiref.compatibility import guarantee_single_callable\nfrom asgiref.typing import (\n  ASGIApplication,\n  ASGIReceiveCallable,\n  ASGISendCallable,\n  ASGISendEvent,\n  Scope,\n)\nfrom zenml.config import DeploymentSettings, MiddlewareSpec\nasync def request_id_middleware(\n  app: ASGIApplication,\n  scope: Scope,\n  receive: ASGIReceiveCallable,\n  send: ASGISendCallable,\n  header_name: str = \"x-request-id\",\n) -> None:\n  \"\"\"ASGI function middleware that ensures a correlation ID header exists.\"\"\"\n  app = guarantee_single_callable(app)\n  if scope[\"type\"] != \"http\":\n    await app(scope, receive, send)\n    return\n  # Reuse existing request ID if present; otherwise generate one\n  request_id = None\n  for k, v in scope.get(\"headers\", []):\n    if k.decode().lower() == header_name:\n      request_id = v.decode()\n      break\n  if not request_id:\n    request_id = str(uuid.uuid4())\n  async def send_wrapper(message: ASGISendEvent) -> None:\n    if message[\"type\"] == \"http.response.start\":\n      headers = list(message.get(\"headers\", []))\n      headers.append((header_name.encode(), request_id.encode()))\n      message = {**message, \"headers\": headers}\n    await send(message)\n  await app(scope, receive, send_wrapper)\nsettings = DeploymentSettings(\n  custom_middlewares=[\n    MiddlewareSpec(\n      middleware=request_id_middleware,\n      order=5,\n      init_kwargs={\"header_name\": \"x-request-id\"},\n    ),\n  ]\n)\n```\n\n\n  1. a FastAPI/Starlette-native middleware that adds GZIP support, implemented as a native middleware:\n\n\n\n\nCopy```\nfrom starlette.middleware.gzip import GZipMiddleware\nfrom zenml.config import DeploymentSettings, MiddlewareSpec\nsettings = DeploymentSettings(\n  custom_middlewares=[\n    MiddlewareSpec(\n      middleware=GZipMiddleware,\n      native=True,\n      order=20,\n      extra_kwargs={\"minimum_size\": 1024},\n    ),\n  ]\n)\n```\n\n\n#### \n\nApp extensions\n\nApp extensions are pluggable components that are running as part of the ASGI application factory that can install complex, possibly framework-specific structures. The following are usual scenarios for using a full-blown extension instead of endpoints/middleware:\n\n  * Advanced authentication and authorization\n\n    * install org-wide dependencies (e.g., OAuth/OIDC auth, RBAC guards)\n\n    * register custom exception handlers for uniform error envelopes\n\n    * augment OpenAPI with security schemes and per-route security policies\n\n  * Multi-tenant and routing topology\n\n    * programmatically include routers per tenant/region/version\n\n    * mount sub-apps for internal admin vs public APIs under different prefixes\n\n    * dynamic route rewrites/switches for blue/green or canary rollouts\n\n  * Observability and platform integration\n\n    * wire OpenTelemetry instrumentation at the app level (tracer/meter providers)\n\n    * register global request/response logging with redaction policies\n\n    * expose or mount vendor-specific observability apps (e.g., Prometheus)\n\n  * LLM agent control plane\n\n    * attach a tool registry/router and lifecycle hooks for tools\n\n    * register guardrail handlers and policy engines across routes\n\n    * install runtime prompt/template catalogs and index management routers\n\n  * API ergonomics and governance\n\n    * reshape OpenAPI (tags, servers, components) and versioned docs\n\n    * global response model wrapping, pagination conventions, error mappers\n\n    * maintenance-mode switch and graceful-drain controls at the app level\n\n\n\n\nApp extensions support multiple definition modes (see code examples below):\n\n  1. Extension class - a class that implements the `BaseAppExtension` abstract class. The class constructor must accept any keyword arguments and the `install` method must accept an `app_runner` argument of type `BaseDeploymentAppRunner`.\n\n  2. Extension callable - a callable that takes the `app_runner` argument of type `BaseDeploymentAppRunner`.\n\n\n\n\nBoth classes and callables must take in an `app_runner` argument of type `BaseDeploymentAppRunner`. This is the application factory that is responsible for building the ASGI application. You can use it to access information such as:\n\n  * the ASGI application instance that is being built\n\n  * the deployment service instance that is being deployed\n\n  * the `DeploymentResponse` object itself, which also contains details about the snapshot, pipeline, etc.\n\n\n\n\nDefinitions can be provided as Python objects or as loadable source path strings.\n\nThe extensions are summoned to take part in the ASGI application building process near the end of the initialization - after the ASGI app has been built according to the deployment configuration settings.\n\nThe example below installs API key authentication at the FastAPI application level, attaches the dependency to selected routes, registers an auth error handler, and augments the OpenAPI schema with the security scheme.\n\nCopy```\nfrom __future__ import annotations\nfrom typing import Literal, Sequence, Set\nfrom fastapi import FastAPI, HTTPException, Request, status\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.responses import JSONResponse\nfrom fastapi.security import APIKeyHeader\nfrom zenml.config import AppExtensionSpec, DeploymentSettings\nfrom zenml.deployers.server.app import BaseDeploymentAppRunner\nfrom zenml.deployers.server.extensions import BaseAppExtension\nclass FastAPIAuthExtension(BaseAppExtension):\n  \"\"\"Install API key auth and OpenAPI security on a FastAPI app.\"\"\"\n  def __init__(\n    self,\n    scheme: Literal[\"api_key\"] = \"api_key\",\n    header_name: str = \"x-api-key\",\n    valid_keys: Sequence[str] | None = None,\n  ) -> None:\n    self.scheme = scheme\n    self.header_name = header_name\n    self.valid_keys: Set[str] = set(valid_keys or [])\n  def install(self, app_runner: BaseDeploymentAppRunner) -> None:\n    app = app_runner.asgi_app\n    if not isinstance(app, FastAPI):\n      raise RuntimeError(\"FastAPIAuthExtension requires FastAPI\")\n    api_key_header = APIKeyHeader(\n      name=self.header_name, auto_error=True\n    )\n    # Find endpoints that have auth_required=True\n    protected_endpoints = [\n      endpoint.path\n      for endpoint in app_runner.endpoints\n      if endpoint.auth_required\n    ]\n    @app.middleware(\"http\")\n    async def api_key_guard(request: Request, call_next):\n      if request.url.path in protected_endpoints:\n        api_key = await api_key_header(request)\n        if api_key not in self.valid_keys:\n          raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid or missing API key\",\n          )\n      return await call_next(request)\n    # Auth error handler\n    @app.exception_handler(HTTPException)\n    async def auth_exception_handler(\n      _, exc: HTTPException\n    ) -> JSONResponse:\n      if exc.status_code == status.HTTP_401_UNAUTHORIZED:\n        return JSONResponse(\n          status_code=exc.status_code,\n          content={\"detail\": exc.detail},\n          headers={\"WWW-Authenticate\": \"ApiKey\"},\n        )\n      return JSONResponse(\n        status_code=exc.status_code, content={\"detail\": exc.detail}\n      )\n    # OpenAPI security\n    def custom_openapi() -> dict:\n      if app.openapi_schema:\n        return app.openapi_schema # type: ignore[return-value]\n      openapi_schema = get_openapi(\n        title=app.title,\n        version=app.version if app.version else \"0.1.0\",\n        description=app.description,\n        routes=app.routes,\n      )\n      components = openapi_schema.setdefault(\"components\", {})\n      security_schemes = components.setdefault(\"securitySchemes\", {})\n      security_schemes[\"ApiKeyAuth\"] = {\n        \"type\": \"apiKey\",\n        \"in\": \"header\",\n        \"name\": self.header_name,\n      }\n      openapi_schema[\"security\"] = [{\"ApiKeyAuth\": []}]\n      app.openapi_schema = openapi_schema\n      return openapi_schema\n    app.openapi = custom_openapi # type: ignore[assignment]\nsettings = DeploymentSettings(\n  app_extensions=[\n    AppExtensionSpec(\n      extension=(\n        \"my_project.extensions.FastAPIAuthExtension\"\n      ),\n      extension_kwargs={\n        \"scheme\": \"api_key\",\n        \"header_name\": \"x-api-key\",\n        \"valid_keys\": [\"secret-1\", \"secret-2\"],\n      },\n    )\n  ]\n)\n```\n\n\n### \n\nImplementation customizations for advanced use cases\n\nFor cases where you need deeper control over how the ASGI app is created or how the deployment logic is implemented, you can swap/extend the core components using the following `DeploymentSettings` fields:\n\n  * `deployment_app_runner_flavor` and `deployment_app_runner_kwargs` let you choose or extend the app runner that constructs and runs the ASGI app. This needs to be set to a subclass of `BaseDeploymentAppRunnerFlavor`, which is basically a descriptor of an app runner implementation that itself is a subclass of `BaseDeploymentAppRunner`.\n\n  * `deployment_service_class` and `deployment_service_kwargs` let you provide your own deployment service to customize the pipeline deployment logic. This needs to be set to a subclass of `BasePipelineDeploymentService`.\n\n\n\n\nBoth accept loadable sources or objects. We cover how to implement custom runner flavors and services in a dedicated guide.\n\nPreviousPipeline DeploymentsNextContainerization\n\nLast updated 1 month ago\n\nWas this helpful?\n",
    "summary": null,
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/deployment/deployment_settings",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#deployment-servers-and-asgi-apps",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#configuration-overview",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#basic-customization-options",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#advanced-customization-options",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#implementation-customizations-for-advanced-use-cases",
        "https://docs.zenml.io/concepts",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#python-configuration",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#yaml-configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/configuration",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#application-metadata",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#default-url-paths-endpoints-and-middleware",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#static-files-single-page-applications",
        "https://docs.zenml.io/concepts/steps_and_pipelines/sources#source-root",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#cors",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#secure-headers",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#startup-and-shutdown-hooks",
        "https://docs.zenml.io/concepts/deployment#deployment-initialization-cleanup-and-state",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#uvicorn-and-threading",
        "https://docs.zenml.io/stacks/stack-components/deployers/docker",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#custom-endpoints",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#custom-middleware",
        "https://docs.zenml.io/concepts/deployment/deployment_settings#app-extensions",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/?utm_source=content&utm_medium=trademark&utm_campaign=5aBlTJNbVDkrxJp7J1J9",
        "https://github.com/zenml-io/zenml/blob/main/docs/book/how-to/deployment/deployment_settings/README.md#secure-headers"
    ]
}