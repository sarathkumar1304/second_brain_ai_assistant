{
    "id": "815cb982df45fa6c4a222cb1cddadee3",
    "metadata": {
        "id": "815cb982df45fa6c4a222cb1cddadee3",
        "url": "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features",
        "title": "Advanced Features | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Advanced features and capabilities of ZenML pipelines and steps",
            "keywords": null,
            "author": null,
            "og:title": "Advanced Features | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Advanced features and capabilities of ZenML pipelines and steps",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/yCpgbh5rCg5nqV1XtNim",
            "twitter:card": "summary_large_image",
            "twitter:title": "Advanced Features | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Advanced features and capabilities of ZenML pipelines and steps",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/yCpgbh5rCg5nqV1XtNim"
        }
    },
    "content": "`Ctrl``k`\n\nGitBook AssistantAsk\n\nProductResourcesGitHubStart free\n\nMore\n\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\n\nGitBook Assistant\n\nGitBook Assistant\n\nWorking...Thinking...\n\nGitBook Assistant\n\n##### Good evening\n\nI'm here to help you with the docs.\n\nWhat is this page about?What should I read next?Can you give an example?\n\n`Ctrl``i`\n\nAI Based on your context\n\nSend\n\n  * Getting Started\n\n    * Welcome to ZenML\n    * Installation\n    * Hello World\n    * Your First AI Pipeline\n    * Core Concepts\n    * System Architecture\n  * Deploying ZenML\n\n    * Deploy\n    * Connect\n    * Manage\n  * Concepts\n\n    * Steps & Pipelines\n\n      * Configuration\n      * Scheduling\n      * Logging\n      * YAML Configuration\n      * Source Code and Imports\n      * Advanced Features\n      * Dynamic Pipelines (Experimental)\n\n    * Artifacts\n    * Stack & Components\n    * Service Connectors\n    * Pipeline Snapshots\n    * Pipeline Deployments\n    * Containerization\n    * Code Repositories\n    * Secrets\n    * Environment Variables\n    * Tags\n    * Metadata\n    * Models\n    * Dashboard\n    * Templates\n  * Reference\n\n    * Community & content\n    * Environment Variables\n    * MCP Docs & llms.txt\n    * FAQ\n    * Global settings\n    * Legacy docs\n\n\n\nPowered by GitBook\n\nOn this page\n\n  * Execution Control\n  * Caching\n  * Running Individual Steps\n  * Asynchronous Pipeline Execution\n  * Step Execution Order\n  * Execution Modes\n  * Data & Output Management\n  * Type annotations\n  * Tuple vs multiple outputs\n  * Step output names\n  * Workflow Patterns\n  * Pipeline Composition\n  * Fan-out and Fan-in\n  * Dynamic Fan-out/Fan-in with Snapshots\n  * Custom Step Invocation IDs\n  * Named Pipeline Runs\n  * Error Handling & Reliability\n  * Automatic Step Retries\n  * Monitoring & Notifications\n  * Pipeline and Step Hooks\n  * Accessing Step Context in Hooks\n  * Using Alerter in Hooks\n  * Conclusion\n\n\n\nWas this helpful?\n\nGitBook AssistantAsk\n\n  1. Concepts\n  2. Steps & Pipelines\n\n\n\n# Advanced Features\n\nAdvanced features and capabilities of ZenML pipelines and steps\n\nThis guide covers advanced features and capabilities of ZenML pipelines and steps, allowing you to build more sophisticated machine learning workflows.\n\n## \n\nExecution Control\n\n### \n\nCaching\n\nSteps are automatically cached based on their code, inputs and other factors. When a step runs, ZenML computes a hash of the inputs and checks if a previous run with the same inputs exists. If found, ZenML reuses the outputs instead of re-executing the step.\n\nYou can control caching behavior at the step level:\n\nCopy```\n@step(enable_cache=False)\ndefnon_cached_step():\npass\n```\n\n\nYou can also configure caching at the pipeline level:\n\nCopy```\n@pipeline(enable_cache=False)\ndefmy_pipeline():\n  ...\n```\n\n\nOr modify it after definition:\n\nCopy```\nmy_step.configure(enable_cache=False)\nmy_pipeline.configure(enable_cache=False)\n```\n\n\nFor more information, check out this page.\n\n### \n\nRunning Individual Steps\n\nYou can run a single step directly:\n\nCopy```\nmodel, accuracy = train_classifier(X_train=X_train, y_train=y_train)\n```\n\n\nThis creates an unlisted pipeline run with just that step. If you want to bypass ZenML completely and run the underlying function directly:\n\nCopy```\nmodel, accuracy = train_classifier.entrypoint(X_train=X_train, y_train=y_train)\n```\n\n\nYou can make this the default behavior by setting the `ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK` environment variable to `True`.\n\n### \n\nAsynchronous Pipeline Execution\n\nBy default, pipelines run synchronously, with terminal logs displaying as the pipeline builds and runs. You can change this behavior to run pipelines asynchronously (in the background):\n\nCopy```\nfrom zenml import pipeline\n@pipeline(settings={\"orchestrator\": {\"synchronous\": False}})\ndef my_pipeline():\n  ...\n```\n\n\nAlternatively, you can configure this in a YAML config file:\n\nCopy```\nsettings:\n orchestrator.<STACK_NAME>:\n  synchronous: false\n```\n\n\nYou can also configure the orchestrator to always run asynchronously by setting `synchronous=False` in its configuration.\n\n### \n\nStep Execution Order\n\nBy default, ZenML determines step execution order based on data dependencies. When a step requires output from another step, it automatically creates a dependency.\n\nYou can explicitly control execution order with the `after` parameter:\n\nCopy```\n@pipeline\ndef my_pipeline():\n  step_a_output = step_a()\n  step_b_output = step_b()\n  # step_c will only run after both step_a and step_b complete, even if\n  # it doesn't use their outputs directly\n  step_c(after=[step_a_output, step_b_output])\n  # You can also specify dependencies using the step invocation ID\n  step_d(after=\"step_c\")\n```\n\n\nThis is particularly useful for steps with side effects (like data loading or model deployment) where the data dependency is not explicit.\n\n### \n\nExecution Modes\n\nZenML provides three execution modes that control how your orchestrator behaves when a step fails during pipeline execution. These modes are:\n\n  * `CONTINUE_ON_FAILURE`: The orchestrator continues executing steps that don't depend on any of the failed steps.\n\n  * `STOP_ON_FAILURE`: The orchestrator allows the running steps to complete, but prevents new steps from starting.\n\n  * `FAIL_FAST`: The orchestrator stops the run and any running steps immediately when a failure occurs.\n\n\n\n\nYou can configure the execution mode of your pipeline in several ways:\n\nCopy```\nfrom zenml import pipeline\nfrom zenml.enums import ExecutionMode\n# Use the decorator\n@pipeline(execution_mode=ExecutionMode.CONTINUE_ON_ERROR)\ndef my_pipeline():\n  ...\n# Use the `with_options` method\nmy_pipeline_with_fail_fast = my_pipeline.with_options(\n  execution_mode=ExecutionMode.FAIL_FAST\n)\n# Use the `configure` method\nmy_pipeline.configure(execution_mode=ExecutionMode.STOP_ON_FAILURE)\n```\n\n\nIn the current implementation, if you use the execution mode `STOP_ON_FAILURE`, the token that is associated with your pipeline run stays valid until its leeway runs out (defaults to 1 hour).\n\nAs an example, you can consider a pipeline with this dependency structure:\n\nCopy```\n     ┌─► Step 2 ──► Step 5 ─┐\nStep 1 ──┼─► Step 3 ──► Step 6 ─┼──► Step 8\n     └─► Step 4 ──► Step 7 ─┘\n```\n\n\nIf steps 2, 3, and 4 execute in parallel and step 2 fails:\n\n  * With `FAIL_FAST`: Step 1 finishes → Steps 2,3,4 start → Step 2 fails → Steps 3, 4 are stopped → No other steps get launched\n\n  * With `STOP_ON_FAILURE`: Step 1 finishes → Steps 2,3,4 start → Step 2 fails but Steps 3, 4 complete → Steps 5, 6, 7 are skipped\n\n  * With `CONTINUE_ON_FAILURE`: Step 1 finishes → Steps 2,3,4 start → Step 2 fails, Steps 3, 4 complete → Step 5 skipped (depends on failed Step 2), Steps 6, 7 run normally → Step 8 is skipped as well.\n\n\n\n\nAll three execution modes are currently only supported by the `local`, `local_docker`, and `kubernetes` orchestrator flavors. For any other orchestrator flavor, the default (and only available) behavior is `CONTINUE_ON_FAILURE`. If you would like to see any of the other orchestrators extended to support the other execution modes, reach out to us in Slack.\n\n## \n\nData & Output Management\n\n## \n\nType annotations\n\nYour functions will work as ZenML steps even if you don't provide any type annotations for their inputs and outputs. However, adding type annotations to your step functions gives you lots of additional benefits:\n\n  * **Type validation of your step inputs** : ZenML makes sure that your step functions receive an object of the correct type from the upstream steps in your pipeline.\n\n  * **Better serialization** : Without type annotations, ZenML uses Cloudpickle to serialize your step outputs. When provided with type annotations, ZenML can choose a materializer that is best suited for the output. In case none of the builtin materializers work, you can even write a custom materializer.\n\n\n\n\nZenML provides a built-in CloudpickleMaterializer that can handle any object by saving it with cloudpickle. However, this is not production-ready because the resulting artifacts cannot be loaded when running with a different Python version. In such cases, you should consider building a custom Materializer to save your objects in a more robust and efficient format.\n\nMoreover, using the `CloudpickleMaterializer` could allow users to upload of any kind of object. This could be exploited to upload a malicious file, which could execute arbitrary code on the vulnerable system.\n\nCopy```\nfrom typing import Tuple\nfrom zenml import step\n@step\ndef square_root(number: int) -> float:\n  return number ** 0.5\n# To define a step with multiple outputs, use a `Tuple` type annotation\n@step\ndef divide(a: int, b: int) -> Tuple[int, int]:\n  return a // b, a % b\n```\n\n\nIf you want to make sure you get all the benefits of type annotating your steps, you can set the environment variable `ZENML_ENFORCE_TYPE_ANNOTATIONS` to `True`. ZenML will then raise an exception in case one of the steps you're trying to run is missing a type annotation.\n\n### \n\nTuple vs multiple outputs\n\nIt is impossible for ZenML to detect whether you want your step to have a single output artifact of type `Tuple` or multiple output artifacts just by looking at the type annotation.\n\nWe use the following convention to differentiate between the two: When the `return` statement is followed by a tuple literal (e.g. `return 1, 2` or `return (value_1, value_2)`) we treat it as a step with multiple outputs. All other cases are treated as a step with a single output of type `Tuple`.\n\nCopy```\nfrom zenml import step\nfrom typing import Annotated\nfrom typing import Tuple\n# Single output artifact\n@step\ndef my_step() -> Tuple[int, int]:\n  output_value = (0, 1)\n  return output_value\n# Single output artifact with variable length\n@step\ndef my_step(condition) -> Tuple[int, ...]:\n  if condition:\n    output_value = (0, 1)\n  else:\n    output_value = (0, 1, 2)\n  return output_value\n# Single output artifact using the `Annotated` annotation\n@step\ndef my_step() -> Annotated[Tuple[int, ...], \"my_output\"]:\n  return 0, 1\n# Multiple output artifacts\n@step\ndef my_step() -> Tuple[int, int]:\n  return 0, 1\n# Not allowed: Variable length tuple annotation when using\n# multiple output artifacts\n@step\ndef my_step() -> Tuple[int, ...]:\n  return 0, 1\n```\n\n\n## \n\nStep output names\n\nBy default, ZenML uses the output name `output` for single output steps and `output_0, output_1, ...` for steps with multiple outputs. These output names are used to display your outputs in the dashboard and fetch them after your pipeline is finished.\n\nIf you want to use custom output names for your steps, use the `Annotated` type annotation:\n\nCopy```\nfrom typing import Annotated\nfrom typing import Tuple\nfrom zenml import step\n@step\ndef square_root(number: int) -> Annotated[float, \"custom_output_name\"]:\n  return number ** 0.5\n@step\ndef divide(a: int, b: int) -> Tuple[\n  Annotated[int, \"quotient\"],\n  Annotated[int, \"remainder\"]\n]:\n  return a // b, a % b\n```\n\n\nIf you do not give your outputs custom names, the created artifacts will be named `{pipeline_name}::{step_name}::output` or `{pipeline_name}::{step_name}::output_{i}` in the dashboard. See the documentation on artifact versioning and configuration for more information.\n\n## \n\nWorkflow Patterns\n\n### \n\nPipeline Composition\n\nYou can compose pipelines from other pipelines to create modular, reusable workflows:\n\nCopy```\n@pipeline\ndef data_pipeline(mode: str):\n  if mode == \"train\":\n    data = training_data_loader_step()\n  else:\n    data = test_data_loader_step()\n  processed_data = preprocessing_step(data)\n  return processed_data\n@pipeline\ndef training_pipeline():\n  # Use another pipeline inside this pipeline\n  training_data = data_pipeline(mode=\"train\")\n  model = train_model(data=training_data)\n  test_data = data_pipeline(mode=\"test\")\n  evaluate_model(model=model, data=test_data)\n```\n\n\nPipeline composition allows you to build complex workflows from simpler, well-tested components.\n\n### \n\nFan-out and Fan-in\n\nThe fan-out/fan-in pattern is a common pipeline architecture where a single step splits into multiple parallel operations (fan-out) and then consolidates the results back into a single step (fan-in). This pattern is particularly useful for parallel processing, distributed workloads, or when you need to process data through different transformations and then aggregate the results. For example, you might want to process different chunks of data in parallel and then aggregate the results:\n\nCopy```\nfrom zenml import step, get_step_context, pipeline\nfrom zenml.client import Client\n@step\ndef load_step() -> str:\n  return \"Hello from ZenML!\"\n@step\ndef process_step(input_data: str) -> str:\n  return input_data\n@step\ndef combine_step(step_prefix: str, output_name: str) -> None:\n  run_name = get_step_context().pipeline_run.name\n  run = Client().get_pipeline_run(run_name)\n  # Fetch all results from parallel processing steps\n  processed_results = {}\n  for step_name, step_info in run.steps.items():\n    if step_name.startswith(step_prefix):\n      output = step_info.outputs[output_name][0]\n      processed_results[step_info.name] = output.load()\n  # Combine all results\n  print(\",\".join([f\"{k}: {v}\" for k, v in processed_results.items()]))\n@pipeline(enable_cache=False)\ndef fan_out_fan_in_pipeline(parallel_count: int) -> None:\n  # Initial step (source)\n  input_data = load_step()\n  # Fan out: Process data in parallel branches\n  after = []\n  for i in range(parallel_count):\n    artifact = process_step(input_data, id=f\"process_{i}\")\n    after.append(artifact)\n  # Fan in: Combine results from all parallel branches\n  combine_step(step_prefix=\"process_\", output_name=\"output\", after=after)\nfan_out_fan_in_pipeline(parallel_count=8)\n```\n\n\nThe fan-out pattern allows for parallel processing and better resource utilization, while the fan-in pattern enables aggregation and consolidation of results. This is particularly useful for:\n\n  * Parallel data processing\n\n  * Distributed model training\n\n  * Ensemble methods\n\n  * Batch processing\n\n  * Data validation across multiple sources\n\n  * Hyperparameter tuning\n\n\n\n\nNote that when implementing the fan-in step, you'll need to use the ZenML Client to query the results from previous parallel steps, as shown in the example above, and you can't pass in the result directly.\n\nThe fan-in, fan-out method has the following limitations:\n\n  1. Steps run sequentially rather than in parallel if the underlying orchestrator does not support parallel step runs (e.g. with the local orchestrator)\n\n  2. The number of steps need to be known ahead-of-time, and ZenML does not yet support the ability to dynamically create steps on the fly.\n\n\n\n\n### \n\nDynamic Fan-out/Fan-in with Snapshots\n\nFor scenarios where you need to determine the number of parallel operations at runtime (e.g., based on database queries or dynamic data), you can use snapshots to create a more flexible fan-out/fan-in pattern. This approach allows you to trigger multiple pipeline runs dynamically and then aggregate their results.\n\nCopy```\nfrom typing import List, Optional\nfrom uuid import UUID\nimport time\nfrom zenml import step, pipeline\nfrom zenml.client import Client\n@step\ndef load_relevant_chunks() -> List[str]:\n  \"\"\"Load chunk identifiers from database or other dynamic source.\"\"\"\n  # Example: Query database for chunk IDs\n  # In practice, this could be a database query, API call, etc.\n  return [\"chunk_1\", \"chunk_2\", \"chunk_3\", \"chunk_4\"]\n@step\ndef trigger_chunk_processing(\n  chunks: List[str], \n  snapshot_id: Optional[UUID] = None\n) -> List[UUID]:\n  \"\"\"Trigger multiple pipeline runs for each chunk and wait for completion.\"\"\"\n  client = Client()\n  # Use snapshot ID if provided, otherwise give the pipeline name \n  # of the pipeline you want triggered. Giving the pipeline name\n  # will automatically find the latest snapshot of that pipeline.\n  pipeline_name = None if snapshot_id else \"chunk_processing_pipeline\"\n  # Trigger all chunk processing runs\n  run_ids = []\n  for chunk_id in chunks:\n    run_config = {\n      \"steps\": {\n        \"process_chunk\": {\n          \"parameters\": {\n            \"chunk_id\": chunk_id\n          }\n        }\n      }\n    }\n    run = client.trigger_pipeline(\n      snapshot_name_or_id=snapshot_id,\n      pipeline_name_or_id=pipeline_name,\n      run_configuration=run_config,\n      synchronous=False # Run asynchronously\n    )\n    run_ids.append(run.id)\n  # Wait for all runs to complete\n  print(f\"Waiting for {len(run_ids)} chunk processing runs to complete...\")\n  completed_runs = set() # Cache completed runs to avoid re-fetching\n  while True:\n    # Only check runs that haven't completed yet\n    pending_runs = [run_id for run_id in run_ids if run_id not in completed_runs]\n    for run_id in pending_runs:\n      run = client.get_pipeline_run(run_id)\n      if run.status.is_finished:\n        completed_runs.add(run_id)\n    if len(completed_runs) == len(run_ids):\n      print(\"All chunk processing runs completed!\")\n      break\n    print(f\"Completed: {len(completed_runs)}/{len(run_ids)} runs\")\n    time.sleep(10) # Wait 10 seconds before checking again\n  return run_ids\n@step\ndef aggregate_results(run_ids: List[UUID]) -> dict:\n  \"\"\"Aggregate results from all chunk processing runs.\"\"\"\n  client = Client()\n  aggregated_results = {}\n  failed_runs = []\n  for run_id in run_ids:\n    run = client.get_pipeline_run(run_id)\n    # Check if run succeeded\n    if run.status.value == \"failed\":\n      failed_runs.append({\n        \"run_id\": str(run_id),\n        \"status\": run.status.value,\n      })\n      print(f\"WARNING: Run {run_id} failed with status {run.status.value}\")\n      continue\n    # Extract results from successful runs only\n    if \"process_chunk\" in run.steps:\n      step_run = run.steps[\"process_chunk\"]\n      # Simple assumption: process_chunk step has one output that we can load\n      chunk_result = step_run.output.load()\n      aggregated_results[str(run_id)] = chunk_result\n  # Log summary of results\n  total_runs = len(run_ids)\n  successful_runs = len(aggregated_results)\n  failed_count = len(failed_runs)\n  print(f\"Aggregation complete: {successful_runs}/{total_runs} runs successful\")\n  return {\n    \"successful_results\": aggregated_results,\n    \"failed_runs\": failed_runs,\n    \"summary\": {\n      \"total_runs\": total_runs,\n      \"successful_runs\": successful_runs,\n      \"failed_runs\": failed_count\n    }\n  }\n@pipeline(enable_cache=False)\ndef fan_out_fan_in_pipeline(snapshot_id: Optional[UUID] = None):\n  \"\"\"Fan-out/fan-in pipeline that orchestrates dynamic chunk processing.\"\"\"\n  # Load chunks dynamically at runtime\n  chunks = load_relevant_chunks()\n  # Trigger chunk processing runs and wait for completion\n  run_ids = trigger_chunk_processing(chunks, snapshot_id)\n  # Aggregate results from all runs\n  results = aggregate_results(run_ids)\n  return results\n# Define the chunk processing pipeline that will be triggered\n@step\ndef process_chunk(chunk_id: Optional[str] = None) -> dict:\n  \"\"\"Process a single chunk of data.\"\"\"\n  # Simulate chunk processing\n  print(f\"Processing chunk: {chunk_id}\")\n  return {\n    \"chunk_id\": chunk_id,\n    \"processed_items\": 100,\n    \"status\": \"completed\"\n  }\n@pipeline\ndef chunk_processing_pipeline():\n  \"\"\"Pipeline that processes a single chunk.\"\"\"\n  result = process_chunk()\n  return result\n# Usage example\nif __name__ == \"__main__\":\n  # First, create a snapshot for the chunk processing pipeline\n  # This would typically be done once during setup.\n  # Make sure a remote stack is set before running this\n  snapshot = chunk_processing_pipeline.create_snapshot(\n    name=\"chunk_processing\",\n    description=\"Snapshot for processing individual chunks\"\n  )\n  # Run the fan-out/fan-in pipeline with the snapshot\n  # You can also get the snapshot ID from the dashboard\n  fan_out_fan_in_pipeline(snapshot_id=snapshot.id)\n```\n\n\nThis pattern enables dynamic scaling, true parallelism, and database-driven workflows. Key advantages include fault tolerance and separate monitoring for each chunk. Consider resource management and proper error handling when implementing.\n\n### \n\nCustom Step Invocation IDs\n\nWhen calling a ZenML step as part of your pipeline, it gets assigned a unique **invocation ID** that you can use to reference this step invocation when defining the execution order of your pipeline steps or use it to fetch information about the invocation after the pipeline has finished running.\n\nCopy```\nfrom zenml import pipeline, step\n@step\ndef my_step() -> None:\n  ...\n@pipeline\ndef example_pipeline():\n  # When calling a step for the first time inside a pipeline,\n  # the invocation ID will be equal to the step name -> `my_step`.\n  my_step()\n  # When calling the same step again, the suffix `_2`, `_3`, ... will\n  # be appended to the step name to generate a unique invocation ID.\n  # For this call, the invocation ID would be `my_step_2`.\n  my_step()\n  # If you want to use a custom invocation ID when calling a step, you can\n  # do so by passing it like this. If you pass a custom ID, it needs to be\n  # unique for all the step invocations that happen as part of this pipeline.\n  my_step(id=\"my_custom_invocation_id\")\n```\n\n\n### \n\nNamed Pipeline Runs\n\nIn the output logs of a pipeline run you will see the name of the run:\n\nCopy```\nPipeline run training_pipeline-2023_05_24-12_41_04_576473 has finished in 3.742s.\n```\n\n\nThis name is automatically generated based on the current date and time. To change the name for a run, pass `run_name` as a parameter to the `with_options()` method:\n\nCopy```\ntraining_pipeline = training_pipeline.with_options(\n  run_name=\"custom_pipeline_run_name\"\n)\ntraining_pipeline()\n```\n\n\nPipeline run names must be unique, so if you plan to run your pipelines multiple times or run them on a schedule, make sure to either compute the run name dynamically or include one of the placeholders that ZenML will replace.\n\nThe substitutions for the custom placeholders like `experiment_name` can be set in:\n\n  * `@pipeline` decorator, so they are effective for all steps in this pipeline\n\n  * `pipeline.with_options` function, so they are effective for all steps in this pipeline run\n\n\n\n\nStandard substitutions always available and consistent in all steps of the pipeline are:\n\n  * `{date}`: current date, e.g. `2024_11_27`\n\n  * `{time}`: current time in UTC format, e.g. `11_07_09_326492`\n\n\n\n\nCopy```\ntraining_pipeline = training_pipeline.with_options(\n  run_name=\"custom_pipeline_run_name_{experiment_name}_{date}_{time}\"\n)\ntraining_pipeline()\n```\n\n\n## \n\nError Handling & Reliability\n\n### \n\nAutomatic Step Retries\n\nFor steps that may encounter transient failures (like network issues or resource limitations), you can configure automatic retries:\n\nCopy```\nfrom zenml.config.retry_config import StepRetryConfig\n@step(\n  retry=StepRetryConfig(\n    max_retries=3, # Maximum number of retry attempts\n    delay=10,    # Initial delay in seconds before first retry\n    backoff=2    # Factor by which delay increases after each retry\n  )\n)\ndef unreliable_step():\n  # This step might fail due to transient issues\n  ...\n```\n\n\nIt's important to note that **retries happen at the step level, not the pipeline level**. This means that ZenML will only retry individual failed steps, not the entire pipeline.\n\nWith this configuration, if the step fails, ZenML will:\n\n  1. Wait 10 seconds before the first retry\n\n  2. Wait 20 seconds (10 × 2) before the second retry\n\n  3. Wait 40 seconds (20 × 2) before the third retry\n\n  4. Fail the pipeline if all retries are exhausted\n\n\n\n\nThis is particularly useful for steps that interact with external services or resources.\n\n## \n\nMonitoring & Notifications\n\n### \n\nPipeline and Step Hooks\n\nHooks allow you to execute custom code at specific points in the pipeline or step lifecycle:\n\nCopy```\ndef success_hook():\n  print(f\"Step completed successfully\")\ndef failure_hook(exception: BaseException):\n  print(f\"Step failed with error: {str(exception)}\")\n@step(on_success=success_hook, on_failure=failure_hook)\ndef my_step():\n  return 42\n```\n\n\nThe following conventions apply to hooks:\n\n  * the success hook takes no arguments\n\n  * the failure hook optionally takes a single `BaseException` typed argument\n\n\n\n\nYou can also define hooks at the pipeline level to apply to all steps:\n\nCopy```\n@pipeline(on_failure=failure_hook, on_success=success_hook)\ndef my_pipeline():\n  ...\n```\n\n\nStep-level hooks take precedence over pipeline-level hooks. Hooks are particularly useful for:\n\n  * Sending notifications when steps fail or succeed\n\n  * Logging detailed information about runs\n\n  * Triggering external workflows based on pipeline state\n\n\n\n\n### \n\nAccessing Step Context in Hooks\n\nYou can access detailed information about the current run using the step context:\n\nCopy```\nfrom zenml import step, get_step_context\ndef on_failure(exception: BaseException):\n  context = get_step_context()\n  print(f\"Failed step: {context.step_run.name}\")\n  print(f\"Parameters: {context.step_run.config.parameters}\")\n  print(f\"Exception: {type(exception).__name__}: {str(exception)}\")\n  # Access pipeline information\n  print(f\"Pipeline: {context.pipeline_run.name}\")\n@step(on_failure=on_failure)\ndef my_step(some_parameter: int = 1):\n  raise ValueError(\"My exception\")\n```\n\n\n### \n\nUsing Alerter in Hooks\n\nYou can use the Alerter stack component to send notifications when steps fail or succeed:\n\nCopy```\nfrom zenml import get_step_context\nfrom zenml.client import Client\ndef on_failure():\n  step_name = get_step_context().step_run.name\n  Client().active_stack.alerter.post(f\"{step_name} just failed!\")\n```\n\n\nZenML provides built-in alerter hooks for common scenarios:\n\nCopy```\nfrom zenml.hooks import alerter_success_hook, alerter_failure_hook\n@step(on_failure=alerter_failure_hook, on_success=alerter_success_hook)\ndef my_step():\n  ...\n```\n\n\n## \n\nConclusion\n\nThese advanced features provide powerful capabilities for building sophisticated machine learning workflows in ZenML. By leveraging these features, you can create pipelines that are more robust, maintainable, and flexible.\n\nSee also:\n\n  * Steps & Pipelines - Core building blocks\n\n  * YAML Configuration - YAML configuration\n\n\n\n\nPreviousSource Code and ImportsNextDynamic Pipelines (Experimental)\n\nLast updated 1 month ago\n\nWas this helpful?\n",
    "summary": null,
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/steps_and_pipelines/configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/scheduling",
        "https://docs.zenml.io/concepts/steps_and_pipelines/logging",
        "https://docs.zenml.io/concepts/steps_and_pipelines/yaml_configuration",
        "https://docs.zenml.io/concepts/steps_and_pipelines/sources",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features",
        "https://docs.zenml.io/concepts/steps_and_pipelines/dynamic_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#execution-control",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#caching",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#running-individual-steps",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#asynchronous-pipeline-execution",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#step-execution-order",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#execution-modes",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#data-and-output-management",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#type-annotations",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#tuple-vs-multiple-outputs",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#step-output-names",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#workflow-patterns",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#pipeline-composition",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#fan-out-and-fan-in",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#dynamic-fan-out-fan-in-with-snapshots",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#custom-step-invocation-ids",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#named-pipeline-runs",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#error-handling-and-reliability",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#automatic-step-retries",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#monitoring-and-notifications",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#pipeline-and-step-hooks",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#accessing-step-context-in-hooks",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#using-alerter-in-hooks",
        "https://docs.zenml.io/concepts/steps_and_pipelines/advanced_features#conclusion",
        "https://docs.zenml.io/concepts",
        "https://docs.zenml.io/user-guides/starter-guide/cache-previous-executions",
        "https://docs.zenml.io/user-guides/best-practices/keep-your-dashboard-server-clean#unlisted-runs",
        "https://zenml.io/slack-invite",
        "https://docs.zenml.io/getting-started/core-concepts#materializers",
        "https://docs.zenml.io/how-to/data-artifact-management/handle-data-artifacts/handle-custom-data-types",
        "https://sdkdocs.zenml.io/latest/core_code_docs/core-materializers.html#zenml.materializers.cloudpickle_materializer",
        "https://docs.zenml.io/how-to/data-artifact-management/handle-data-artifacts/handle-custom-data-types#custom-materializers",
        "https://docs.zenml.io/user-guides/tutorial/fetching-pipelines",
        "https://docs.zenml.io/user-guides/starter-guide/manage-artifacts",
        "https://docs.zenml.io/user-guides/tutorial/trigger-pipelines-from-external-systems",
        "https://docs.zenml.io/component-guide/alerters",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/?utm_source=content&utm_medium=trademark&utm_campaign=5aBlTJNbVDkrxJp7J1J9",
        "https://github.com/cloudpipe/cloudpickle"
    ]
}