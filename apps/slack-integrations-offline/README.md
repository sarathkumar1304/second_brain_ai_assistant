# ğŸš€ Slack Integrations Offline Pipeline

This guide will help you set up and run the offline ML pipeline for Slack Integrations. You'll build the data infrastructure that crawls documentation, processes it through ETL pipelines, and generates vector embeddings for RAG-based question answering.

# ğŸ“‘ Table of Contents

- [ğŸ“‹ Prerequisites](#-prerequisites)
- [ğŸ¯ Getting Started](#-getting-started)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ—ï¸ Set Up Your Local Infrastructure](#-set-up-your-local-infrastructure)
- [âš¡ï¸ Running the Code for Each Module](#ï¸-running-the-code-for-each-module)

# ğŸ“‹ Prerequisites

## Local Tools

For all the modules, you'll need the following tools installed locally:

| Tool | Version | Purpose | Installation Link |
|------|---------|---------|------------------|
| Python | 3.12 | Programming language runtime | [Download](https://www.python.org/downloads/) |
| uv | â‰¥ 0.4.30 | Python package installer and virtual environment manager | [Download](https://github.com/astral-sh/uv) |
| Git | â‰¥2.44.0 | Version control | [Download](https://git-scm.com/downloads) |
| Docker | â‰¥27.4.0 | Containerization platform | [Download](https://www.docker.com/get-started/) |

<details>
<summary><b>ğŸ“Œ Windows users also need to install WSL for ZenML to run (Click to expand)</b></summary>

You need to **install WSL**, which will install a Linux kernel on your Windows machine to run ZenML pipelines, because ZenML currently does not support windows. 

ğŸ”— [Follow this guide to install WSL](https://www.youtube.com/watch?v=YByZ_sOOWsQ).
</details>

## Cloud Services

Also, the pipeline requires access to these cloud services. The authentication to these services is done by adding the corresponding environment variables to the `.env` file:

| Service | Purpose | Cost | Environment Variable | Setup Guide |
|---------|---------|------|----------------------|-------------|
| [OpenAI API](https://openai.com/index/openai-api/) | LLM API | Pay-per-use | `OPENAI_API_KEY` | [Quick Start Guide](https://platform.openai.com/docs/quickstart) |

When working locally, the infrastructure is set up using Docker. Thus, you can use the default values found in the [config.py](src/slack_integrations_offline/config.py) file for all the infrastructure-related environment variables.

But, in case you want to deploy the code, you'll need to setup the following services with their corresponding environment variables:

| Service | Purpose | Cost | Required Credentials | Setup Guide |
|---------|---------|------|---------------------|-------------| 
| [MongoDB](https://rebrand.ly/second-brain-course-mongodb) | document database (with vector search) | Free tier | `MONGODB_URI` | 1. [Create a free MongoDB Atlas account](https://rebrand.ly/second-brain-course-mongodb-setup-1) <br> 2. [Create a Cluster](https://rebrand.ly/second-brain-course-mongodb-setup-2) </br> 3. [Add a Database User](https://rebrand.ly/second-brain-course-mongodb-setup-3) </br> 4. [Configure a Network Connection](https://rebrand.ly/second-brain-course-mongodb-setup-4) |

# ğŸ¯ Getting Started

## 1. Clone the Repository

Start by cloning the repository and navigating to the project directory:
```
git clone 
cd slack_integrations
```

## 2. Installation

First deactivate any active virtual environment and move to the `slack-integrations-offline` directory:
```bash
deactivate
cd apps/slack-integrations-offline
```

To install the dependencies and activate the virtual environment, run the following commands:

```bash
uv venv .venv-offline
source .venv-offline/bin/activate
uv sync
```

We use [Crawl4AI](https://github.com/unclecode/crawl4ai) for crawling. To finish setting it up you have to run some post-installation setup commands (more on why this is needed in their [docs](https://github.com/unclecode/crawl4ai)):
```bash
# Run post-installation setup
crawl4ai-setup

# Verify your installation

```

After running the doctor command, you should see something like this:
```console
[INIT].... â†’ Running Crawl4AI health check...
[INIT].... â†’ Crawl4AI 0.4.247
[TEST].... â„¹ Testing crawling capabilities...
[EXPORT].. â„¹ Exporting PDF and taking screenshot took 0.84s
[FETCH]... â†“ https://crawl4ai.com... | Status: True | Time: 3.91s
[SCRAPE].. â—† Processed https://crawl4ai.com... | Time: 11ms
[COMPLETE] â— https://crawl4ai.com... | Status: True | Total: 3.92s
[COMPLETE] â— âœ… Crawling test passed!
```
[More on installing Crawl4AI](https://docs.crawl4ai.com/core/installation/)

## 3. Environment Configuration

Before running any command, you have to set up your environment:
1. Create your environment file:
   ```bash
   cp .env.example .env
   ```
2. Open `.env` and configure the required credentials following the inline comments and the recommendations from the [Cloud Services](#-prerequisites) section.

# ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ configs/                         # ZenML configuration files
â”œâ”€â”€ pipelines/                       # ZenML ML pipeline definitions
â”œâ”€â”€ src/slack_integrations_offline/  # Main package directory
â”‚   â”œâ”€â”€ applications/                # Application layer
â”‚   â”œâ”€â”€ domain/                      # Domain layer
â”‚   â”œâ”€â”€ infrastructure/              # Infrastructure layer
â”‚   â”œâ”€â”€ rag/                         # RAG layer
â”‚   â”œâ”€â”€ config.py                    # Configuration settings
â”‚   â””â”€â”€ utils.py                     # Utility functions
â”œâ”€â”€ steps/                           # ZenML pipeline steps
â”œâ”€â”€ tools/                           # Entrypoint scripts that use the Python package
â”œâ”€â”€ .env.example                     # Environment variables template
â”œâ”€â”€ .python-version                  # Python version specification
â”œâ”€â”€ Makefile                         # Project commands
â””â”€â”€ pyproject.toml                   # Project dependencies
```

# ğŸ—ï¸ Set Up Your Local Infrastructure

We use Docker to set up the local infrastructure (ZenML, MongoDB).

> [!WARNING]
> Before running the command below, ensure you do not have any processes running on port `27017` (MongoDB) and `8237` (ZenML).

To start the Docker infrastructure, run:
```bash
make local-infrastructure-up
```

To stop the Docker infrastructure, run:
```bash
make local-infrastructure-down
```

> [!NOTE]
> To visualize the raw and RAG data from MongoDB, we recommend using [MongoDB Compass](https://rebrand.ly/second-brain-course-mongodb-compass) or Mongo's official IDE plugin (e.g., `MongoDB for VS Code`). To connect to the working MongoDB instance, use the `MONGODB_URI` value from the `.env` file or found inside the [config.py](src/slack_integrations_offline/config.py) file.

[More on setting up `MongoDB for VS Code`](https://youtu.be/gFjpv-nZO0U?si=eGxPeqGN2NfIZg0H)

![mongodb_for_vscode.png](../../static/mongodb_for_vscode.png)

[More on setting up `MongoDB Compass`](https://youtu.be/sSoVyHap3HY?si=IZd_F-hUZfN6-JPk)

![monogdb_compass.png](../../static/monogdb_compass.png)

# âš¡ï¸ Running the Code for Each Module

Before running any module first follow these steps to clear the langchain with mongodb dependency issues:

### Step 1: 

Navigate to `.venv-offline/lib/python3.12/site-packages/langchain_mongodb/retrievers/__init__.py` then comment `MongoDBAtlasParentDocumentRetriever` and `MongoDBAtlasSelfQueryRetriever` imports.

### Step 2: 

Navigate to `.venv-offline/lib/python3.12/site-packages/langchain_mongodb/retrievers/parent_document.py` then comment the entire parent document file.

## Module 1: Collect crawl data

Run the below command to collect the crawled data from the documentation web pages.
```bash
ur run python -m tools.run --run-collect-crawl-data-pipeline
```

> [!IMPORTANT]
> If running `make collect-crawl-data` fails, type `https://support-public-data.s3.us-east-1.amazonaws.com/slack_integrations/crawled/crawled.zip` in your browser to download the dataset manually. Unzip `crawled.zip` and place it under the `data` directory as follows: `data/crawled` (create the `data` directory if it doesn't exist).

Running criteria:
- Running costs: $0
- Running time: ~2 minutes

![collect_crawl_data_pipeline.png](../../static/collect_crawl_data_pipeline.png)

## Module 2: ETL pipeline

Run the ETL pipeline to generate summaries and ingest into MongoDB:
```bash
uv run python -m tools.run --run-etl-pipeline
```

Running criteria:
- Running costs: ~$0.05
- Running time: ~2 minutes

![etl_pipeline.png](../../static/etl_pipeline.png)

## Module 3: Compute RAG pipeline

Run the Compute RAG pipeline to fetch raw documents from MongoDB then Chunk these documents, generate embeddings, and load them into MongoDB with vector index.
```bash
uv run python -m tools.run --run-compute-rag-pipeline
```

Running criteria:
- Running costs: ~$0.05
- Running time: ~3 minutes

![compute_rag_pipeline.png](../../static/compute_rag_pipeline.png)

